{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cold Start SFT + GRPO Training for Gemma 3\n",
        "\n",
        "## A Complete Guide to Training Reasoning Models on Kaggle TPU\n",
        "\n",
        "[![Open in Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/google/tunix/blob/main/examples/grpo_gemma.ipynb)\n",
        "\n",
        "---\n",
        "\n",
        "### What This Notebook Does\n",
        "\n",
        "This notebook implements a **two-stage training pipeline** to enhance the reasoning capabilities of the Gemma 3 1B-IT model:\n",
        "\n",
        "1. **Stage 1: Cold Start SFT (Supervised Fine-Tuning)**\n",
        "   - Uses `PeftTrainer` from Tunix (following qlora_gemma approach)\n",
        "   - Teaches the model the correct output format: `<reasoning>...</reasoning><answer>...</answer>`\n",
        "   - Plants a \"reasoning template\" into the model using high-quality Chain-of-Thought data\n",
        "   - Prevents the model from producing unreadable or chaotic outputs during RL training\n",
        "\n",
        "2. **Stage 2: GRPO (Group Relative Policy Optimization)**\n",
        "   - Reinforces the model's reasoning abilities through reward-based learning\n",
        "   - Uses multiple reward functions to guide the model toward correct answers and proper formatting\n",
        "   - More memory-efficient than traditional PPO (no need for a separate value model)\n",
        "\n",
        "### Why Two Stages?\n",
        "\n",
        "Research from DeepSeek-R1 shows that pure RL training can cause models to:\n",
        "- Mix languages randomly\n",
        "- Produce unstructured, hard-to-read outputs\n",
        "- Explore inefficiently due to lack of initial guidance\n",
        "\n",
        "The **Cold Start SFT** stage solves these problems by giving the model a \"template\" for how to reason, before we use GRPO to strengthen that reasoning.\n",
        "\n",
        "### Output Format\n",
        "\n",
        "The trained model will produce outputs in this format:\n",
        "```\n",
        "<reasoning>model_thinking_trace</reasoning>\n",
        "<answer>model_answer</answer>\n",
        "```\n",
        "\n",
        "### Hardware Requirements\n",
        "- **Kaggle TPU v6e-1** (recommended) or similar TPU configuration\n",
        "- Training time: ~9 hours for full pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Environment Setup\n",
        "\n",
        "### Understanding the Dependencies\n",
        "\n",
        "Before we start training, we need to install several key libraries:\n",
        "\n",
        "- **tunix**: Google's library for training Gemma models on TPU with JAX\n",
        "- **qwix**: Provides LoRA (Low-Rank Adaptation) functionality for efficient fine-tuning\n",
        "- **flax**: Neural network library for JAX\n",
        "- **grain**: Data loading library optimized for JAX\n",
        "- **transformers**: For tokenizer and model utilities\n",
        "\n",
        "**Important**: After running the installation cell, you may need to restart the kernel for the changes to take effect.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 1: Install Required Packages\n",
        "# ============================================================================\n",
        "# This cell installs all necessary libraries for training.\n",
        "# RESTART THE KERNEL AFTER THIS CELL COMPLETES (for Colab users)\n",
        "\n",
        "import importlib.util\n",
        "\n",
        "def check_package(name):\n",
        "    \"\"\"Check if a package is installed.\"\"\"\n",
        "    return importlib.util.find_spec(name) is not None\n",
        "\n",
        "# Check for key packages - tunix is the most critical one\n",
        "TUNIX_INSTALLED = check_package('tunix')\n",
        "QWIX_INSTALLED = check_package('qwix')\n",
        "\n",
        "if not TUNIX_INSTALLED or not QWIX_INSTALLED:\n",
        "    print(\"Installing required packages... This may take a few minutes.\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Core dependencies\n",
        "    %pip install -q python-dotenv\n",
        "    %pip install -q kagglehub\n",
        "    %pip install -q ipywidgets\n",
        "    %pip install -q tensorflow\n",
        "    %pip install -q tensorflow_datasets\n",
        "    %pip install -q tensorboardX\n",
        "    %pip install -q transformers\n",
        "    %pip install -q grain\n",
        "    \n",
        "    # JAX and related libraries (for TPU training)\n",
        "    %pip install -q git+https://github.com/jax-ml/jax\n",
        "    \n",
        "    # Google's training libraries (CRITICAL - these are the main packages)\n",
        "    %pip install git+https://github.com/google/tunix  # Main training framework\n",
        "    %pip install git+https://github.com/google/qwix   # LoRA support\n",
        "    \n",
        "    # Flax update (required for NNX support)\n",
        "    %pip uninstall -q flax -y\n",
        "    %pip install git+https://github.com/google/flax\n",
        "    \n",
        "    # Data and utilities\n",
        "    %pip install -q huggingface_hub\n",
        "    %pip install -q datasets\n",
        "    %pip install -q 'numpy>2'\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Installation complete!\")\n",
        "    print(\"IMPORTANT: Please RESTART the kernel before continuing.\")\n",
        "    print(\"=\" * 60)\n",
        "else:\n",
        "    print(\"All required packages are already installed.\")\n",
        "    print(f\"  - tunix: {'OK' if TUNIX_INSTALLED else 'MISSING'}\")\n",
        "    print(f\"  - qwix:  {'OK' if QWIX_INSTALLED else 'MISSING'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Authentication Setup\n",
        "\n",
        "This cell handles authentication for:\n",
        "- **Hugging Face**: To download the Gemma model\n",
        "- **Kaggle**: For dataset access\n",
        "- **Weights & Biases (optional)**: For experiment tracking\n",
        "\n",
        "You need to set up your API keys as environment variables or secrets before running this cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 2: Authentication and Service Login\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import kagglehub\n",
        "\n",
        "# Detect if running in Colab or Kaggle environment\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    USE_COLAB = True\n",
        "    \n",
        "    # WandB has issues with Colab, so we disable it\n",
        "    %pip uninstall -q wandb -y\n",
        "    \n",
        "    # Load credentials from Colab secrets\n",
        "    os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
        "    os.environ[\"KAGGLE_USERNAME\"] = userdata.get(\"KAGGLE_USERNAME\")\n",
        "    os.environ[\"KAGGLE_KEY\"] = userdata.get(\"KAGGLE_KEY\")\n",
        "    print(\"Running in Google Colab environment\")\n",
        "    \n",
        "except ImportError:\n",
        "    USE_COLAB = False\n",
        "    \n",
        "    # Try to load from .env file\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "    print(\"Using environment variables for authentication\")\n",
        "    \n",
        "    # Apply nest_asyncio for Jupyter compatibility\n",
        "    import nest_asyncio\n",
        "    nest_asyncio.apply()\n",
        "    print(\"nest_asyncio applied for async compatibility\")\n",
        "    \n",
        "    # Setup WandB for TPU VM (works better outside Colab)\n",
        "    %pip install -q wandb\n",
        "    import wandb\n",
        "    if \"WANDB_API_KEY\" in os.environ and os.environ[\"WANDB_API_KEY\"]:\n",
        "        wandb.login(key=os.environ[\"WANDB_API_KEY\"])\n",
        "        print(\"Weights & Biases login successful\")\n",
        "    else:\n",
        "        print(\"WANDB_API_KEY not found. Skipping W&B login.\")\n",
        "\n",
        "# Kaggle authentication\n",
        "if \"KAGGLE_USERNAME\" not in os.environ or \"KAGGLE_KEY\" not in os.environ:\n",
        "    print(\"Kaggle credentials not found. Please login manually:\")\n",
        "    kagglehub.login()\n",
        "\n",
        "# Hugging Face authentication\n",
        "if \"HF_TOKEN\" in os.environ and os.environ[\"HF_TOKEN\"]:\n",
        "    hf_token = os.environ[\"HF_TOKEN\"]\n",
        "    !huggingface-cli login --token \"$hf_token\"\n",
        "else:\n",
        "    print(\"HF_TOKEN not found. Please set it for model download.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Libraries\n",
        "\n",
        "Now we import all the necessary libraries. Here's what each major import does:\n",
        "\n",
        "- **jax, jnp**: Core library for accelerated numerical computing on TPU/GPU\n",
        "- **flax.nnx**: Neural network library with the new NNX API\n",
        "- **tunix**: Google's library for Gemma model training\n",
        "- **qwix**: LoRA (Low-Rank Adaptation) for efficient fine-tuning\n",
        "- **grain**: Efficient data loading for JAX\n",
        "- **optax**: Gradient transformation and optimization library\n",
        "- **peft_trainer**: Tunix's built-in SFT trainer (following qlora_gemma approach)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 3: Import All Required Libraries\n",
        "# ============================================================================\n",
        "\n",
        "import functools\n",
        "from pprint import pprint\n",
        "import re\n",
        "import sys\n",
        "import csv\n",
        "import json\n",
        "import shutil\n",
        "import logging\n",
        "\n",
        "# JAX ecosystem\n",
        "from flax import nnx\n",
        "import grain\n",
        "import humanize\n",
        "from huggingface_hub import snapshot_download\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import kagglehub\n",
        "import numpy as np\n",
        "import optax\n",
        "from orbax import checkpoint as ocp\n",
        "from pathlib import Path\n",
        "import qwix\n",
        "import tensorflow_datasets as tfds\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Tunix - Google's Gemma training library\n",
        "from tunix.generate import sampler as sampler_lib\n",
        "from tunix.generate import tokenizer_adapter as tokenizer_lib\n",
        "from tunix.models.gemma3 import model as gemma_lib\n",
        "from tunix.models.gemma3 import params_safetensors as params_safetensors_lib\n",
        "from tunix.models.gemma3 import params as gemma_params\n",
        "from tunix.rl import rl_cluster as rl_cluster_lib\n",
        "from tunix.rl.grpo.grpo_learner import GRPOConfig, GRPOLearner\n",
        "from tunix.rl.rollout import base_rollout\n",
        "from tunix.sft import metrics_logger\n",
        "from tunix.sft import peft_trainer  # SFT trainer from qlora_gemma approach\n",
        "from tunix.sft import utils\n",
        "from tunix.sft.utils import show_hbm_usage\n",
        "\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "print(\"All imports successful!\")\n",
        "print(f\"JAX version: {jax.__version__}\")\n",
        "print(f\"Number of devices available: {len(jax.devices())}\")\n",
        "print(f\"Device type: {jax.devices()[0].device_kind}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Hyperparameter Configuration\n",
        "\n",
        "### Understanding the Key Hyperparameters\n",
        "\n",
        "This section defines all the configuration parameters for both SFT and GRPO training. Let's break down the most important ones:\n",
        "\n",
        "**Model Configuration:**\n",
        "- `MODEL_ID`: The base Gemma model to fine-tune\n",
        "- `RANK` and `ALPHA`: LoRA parameters that control the size of trainable adapters\n",
        "\n",
        "**Training Configuration:**\n",
        "- `MAX_SEQ_LENGTH`: Maximum sequence length for training (longer = more memory)\n",
        "- `LEARNING_RATE`: Step size for gradient updates (too high = unstable, too low = slow)\n",
        "\n",
        "**GRPO-Specific Parameters:**\n",
        "- `NUM_GENERATIONS`: How many responses to generate per prompt for comparison (the \"G\" in GRPO)\n",
        "- `BETA`: KL divergence penalty coefficient (keeps the policy close to reference)\n",
        "- `EPSILON`: Clipping parameter for stable updates (similar to PPO)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 4: Hyperparameter Configuration\n",
        "# ============================================================================\n",
        "# All training parameters are defined here for easy modification.\n",
        "# Adjust these based on your hardware and training requirements.\n",
        "\n",
        "# ===========================================================================\n",
        "# MODEL CONFIGURATION\n",
        "# ===========================================================================\n",
        "MODEL_ID = \"google/gemma-3-1b-it\"  # Base model to fine-tune\n",
        "GEMMA_TOKENIZER_PATH = \"gs://gemma-data/tokenizers/tokenizer_gemma3.model\"\n",
        "\n",
        "# ===========================================================================\n",
        "# DATA PATHS\n",
        "# ===========================================================================\n",
        "TRAIN_DATA_DIR = \"./data/train\"\n",
        "TEST_DATA_DIR = \"./data/test\"\n",
        "SFT_DATA_DIR = \"./data/sft\"  # For cold start SFT data\n",
        "TRAIN_FRACTION = 0.9  # Fraction of data used for training (rest for validation)\n",
        "\n",
        "# ===========================================================================\n",
        "# LoRA CONFIGURATION\n",
        "# ===========================================================================\n",
        "# LoRA (Low-Rank Adaptation) allows us to fine-tune large models efficiently\n",
        "# by only training a small number of additional parameters.\n",
        "RANK = 64       # Rank of the LoRA matrices (higher = more parameters, better quality)\n",
        "ALPHA = 64.0    # Scaling factor for LoRA (typically set equal to RANK)\n",
        "\n",
        "# ===========================================================================\n",
        "# SHARDING CONFIGURATION (for TPU)\n",
        "# ===========================================================================\n",
        "# Configure the mesh for distributed training across TPU cores\n",
        "NUM_TPUS = len(jax.devices())\n",
        "print(f\"Detected {NUM_TPUS} TPU cores\")\n",
        "\n",
        "if NUM_TPUS == 8:\n",
        "    MESH_COUNTS = (1, 4)  # For v3-8 TPU\n",
        "elif NUM_TPUS == 4:\n",
        "    MESH_COUNTS = (1, 4)  # For v4-8 TPU\n",
        "elif NUM_TPUS == 1:\n",
        "    MESH_COUNTS = (1, 1)  # Single device\n",
        "else:\n",
        "    # Default configuration for other setups\n",
        "    MESH_COUNTS = (1, NUM_TPUS)\n",
        "    \n",
        "MESH = [MESH_COUNTS, (\"fsdp\", \"tp\")]\n",
        "\n",
        "# ===========================================================================\n",
        "# DEBUG MODE CONFIGURATION\n",
        "# ===========================================================================\n",
        "# Set to True for fast debugging (reduces training time significantly)\n",
        "DEBUG_MODE = False  # Set to True to reduce steps, batches, and generation length for quick testing\n",
        "\n",
        "# ===========================================================================\n",
        "# SEQUENCE LENGTH CONFIGURATION\n",
        "# ===========================================================================\n",
        "MAX_PROMPT_LENGTH = 256           # Maximum length of input prompts\n",
        "TOTAL_GENERATION_STEPS = 768      # Maximum tokens to generate\n",
        "if DEBUG_MODE:\n",
        "    TOTAL_GENERATION_STEPS = 256  # Reduced for fast debugging\n",
        "MAX_SEQ_LENGTH = MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS\n",
        "\n",
        "# ===========================================================================\n",
        "# COLD START SFT CONFIGURATION (using PeftTrainer)\n",
        "# ===========================================================================\n",
        "SFT_LEARNING_RATE = 2e-4          # Learning rate for SFT (higher than GRPO)\n",
        "SFT_BATCH_SIZE = 2                # Batch size for SFT\n",
        "SFT_MAX_STEPS = 500               # Number of SFT training steps\n",
        "if DEBUG_MODE:\n",
        "    SFT_MAX_STEPS = 10            # Reduced for fast debugging\n",
        "EVAL_EVERY_N_STEPS = 50           # Evaluate every N steps\n",
        "SFT_NUM_EPOCHS = 3                # Number of epochs for SFT\n",
        "\n",
        "# ===========================================================================\n",
        "# GRPO CONFIGURATION (Based on DeepSeek-R1 Paper)\n",
        "# ===========================================================================\n",
        "# Reference: DeepSeek-R1 uses EPSILON=10, BETA=0.001, NUM_GENERATIONS=16\n",
        "# We adjust values for single TPU memory constraints while following their principles.\n",
        "\n",
        "# Generation parameters during GRPO training\n",
        "TEMPERATURE = 1.0     # DeepSeek-R1: 1.0 for RL Stage 1 (high for exploration)\n",
        "TOP_P = 1.0           # Nucleus sampling parameter\n",
        "TOP_K = 50            # Top-k sampling parameter\n",
        "\n",
        "# GRPO algorithm parameters (adjusted from DeepSeek-R1)\n",
        "NUM_GENERATIONS = 4   # DeepSeek-R1 uses 16, reduced for memory\n",
        "if DEBUG_MODE:\n",
        "    NUM_GENERATIONS = 2           # Reduced for fast debugging (fewer generations per prompt)\n",
        "NUM_ITERATIONS = 1    # Number of iterations per batch\n",
        "BETA = 0.001          # KL coefficient (DeepSeek-R1: 0.001, much lower than typical PPO!)\n",
        "EPSILON = 10.0        # Clip ratio (DeepSeek-R1: 10, NOT 0.2 like PPO!)\n",
        "\n",
        "# GRPO training parameters\n",
        "GRPO_LEARNING_RATE = 3e-6         # DeepSeek-R1: 3e-6 ✓\n",
        "GRPO_BATCH_SIZE = 1               # Keep small due to memory constraints\n",
        "GRPO_GRADIENT_ACCUMULATION = 4    # Effective batch: 4 questions * 4 gen = 16 responses\n",
        "GRPO_MAX_STEPS = 2500             # Number of GRPO training steps\n",
        "if DEBUG_MODE:\n",
        "    GRPO_MAX_STEPS = 10           # Reduced for fast debugging (just a few steps to test)\n",
        "GRPO_WARMUP_RATIO = 0.1           # Warmup as fraction of total steps\n",
        "REFERENCE_UPDATE_STEPS = 400      # DeepSeek-R1: Update reference model every 400 steps\n",
        "\n",
        "# ===========================================================================\n",
        "# OPTIMIZER CONFIGURATION\n",
        "# ===========================================================================\n",
        "B1 = 0.9              # Adam beta1\n",
        "B2 = 0.99             # Adam beta2  \n",
        "WEIGHT_DECAY = 0.1    # Weight decay for regularization\n",
        "MAX_GRAD_NORM = 0.1   # Gradient clipping (important for stability)\n",
        "\n",
        "# ===========================================================================\n",
        "# CHECKPOINT CONFIGURATION\n",
        "# ===========================================================================\n",
        "SFT_CKPT_DIR = \"./checkpoints/sft/\"           # SFT checkpoint directory\n",
        "GRPO_CKPT_DIR = \"./checkpoints/grpo/\"         # GRPO checkpoint directory\n",
        "FINAL_MODEL_DIR = \"./checkpoints/final/\"      # Final merged model\n",
        "SAVE_INTERVAL_STEPS = 500                     # Save every N steps\n",
        "MAX_TO_KEEP = 4                               # Maximum checkpoints to keep\n",
        "\n",
        "# ===========================================================================\n",
        "# INFERENCE CONFIGURATION\n",
        "# ===========================================================================\n",
        "GENERATION_CONFIGS = {\n",
        "    \"greedy\": {\"temperature\": None, \"top_k\": 1, \"top_p\": None},      # Deterministic\n",
        "    \"standard\": {\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},    # Balanced\n",
        "    \"creative\": {\"temperature\": 0.85, \"top_k\": 2000, \"top_p\": 1.0},  # More random\n",
        "}\n",
        "\n",
        "print(\"\\nConfiguration loaded successfully!\")\n",
        "print(f\"Model: {MODEL_ID}\")\n",
        "print(f\"LoRA Rank: {RANK}, Alpha: {ALPHA}\")\n",
        "print(f\"SFT Steps: {SFT_MAX_STEPS}, GRPO Steps: {GRPO_MAX_STEPS}\")\n",
        "if DEBUG_MODE:\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"⚠️  DEBUG MODE ENABLED - Fast testing configuration\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"  - SFT Steps: {SFT_MAX_STEPS} (reduced from 500)\")\n",
        "    print(f\"  - GRPO Steps: {GRPO_MAX_STEPS} (reduced from 2500)\")\n",
        "    print(f\"  - Generations per prompt: {NUM_GENERATIONS} (reduced from 4)\")\n",
        "    print(f\"  - Generation length: {TOTAL_GENERATION_STEPS} tokens (reduced from 768)\")\n",
        "    print(\"=\" * 60 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Utility Functions and Special Tokens\n",
        "\n",
        "### Output Format Definition\n",
        "\n",
        "We define a specific output format that the model must learn to use. This format separates:\n",
        "1. **Reasoning**: The model's thought process (between `<reasoning>` and `</reasoning>` tags)\n",
        "2. **Answer**: The final answer (between `<answer>` and `</answer>` tags)\n",
        "\n",
        "**Note**: The data may contain `<think>` or `<think>` markers which we will convert to `<reasoning>` format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 5: Special Tokens and Template Definition\n",
        "# ============================================================================\n",
        "\n",
        "# Define special tokens for structured output\n",
        "reasoning_start = \"<reasoning>\"\n",
        "reasoning_end = \"</reasoning>\"\n",
        "solution_start = \"<answer>\"\n",
        "solution_end = \"</answer>\"\n",
        "\n",
        "# System prompt that instructs the model on the expected output format\n",
        "SYSTEM_PROMPT = f\"\"\"You are given a problem. First, think about the problem \\\n",
        "and provide your reasoning. Place it between {reasoning_start} and \\\n",
        "{reasoning_end}. Then, provide the final answer between {solution_start} and {solution_end}.\"\"\"\n",
        "\n",
        "# Template for formatting prompts (Gemma chat format)\n",
        "TEMPLATE = \"\"\"<start_of_turn>user\n",
        "{system_prompt}\n",
        "\n",
        "{question}<end_of_turn>\n",
        "<start_of_turn>model\n",
        "\"\"\"\n",
        "\n",
        "# Regex pattern to validate output format\n",
        "match_format = re.compile(\n",
        "    rf\"^[\\s]{{0,}}\"\n",
        "    rf\"{reasoning_start}.+?{reasoning_end}.*?\"\n",
        "    rf\"{solution_start}(.+?){solution_end}\"\n",
        "    rf\"[\\s]{{0,}}$\",\n",
        "    flags=re.MULTILINE | re.DOTALL,\n",
        ")\n",
        "\n",
        "# Regex to extract numbers from answers\n",
        "match_numbers = re.compile(\n",
        "    rf\"{solution_start}.*?([\\d\\.]{{1,}})\", \n",
        "    flags=re.MULTILINE | re.DOTALL\n",
        ")\n",
        "\n",
        "# Test the format matching\n",
        "example = f\"{reasoning_start}Let me think step by step...{reasoning_end}{solution_start}42{solution_end}\"\n",
        "print(\"Example output format:\")\n",
        "print(example)\n",
        "print(f\"\\nFormat validation: {'PASS' if match_format.search(example) else 'FAIL'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 6: Utility Functions\n",
        "# ============================================================================\n",
        "\n",
        "def extract_hash_answer(text: str) -> str:\n",
        "    \"\"\"Extract answer from GSM8K format (after #### marker).\"\"\"\n",
        "    if \"####\" not in text:\n",
        "        return None\n",
        "    return text.split(\"####\")[1].strip()\n",
        "\n",
        "\n",
        "def create_directories():\n",
        "    \"\"\"Create all necessary directories for checkpoints and data.\"\"\"\n",
        "    dirs = [TRAIN_DATA_DIR, TEST_DATA_DIR, SFT_DATA_DIR, \n",
        "            SFT_CKPT_DIR, GRPO_CKPT_DIR, FINAL_MODEL_DIR]\n",
        "    for d in dirs:\n",
        "        os.makedirs(d, exist_ok=True)\n",
        "    print(\"All directories created successfully!\")\n",
        "\n",
        "create_directories()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Model Loading\n",
        "\n",
        "### Loading the Base Gemma Model\n",
        "\n",
        "We download the Gemma 3 1B-IT model from Hugging Face and load it using Tunix. The model is:\n",
        "- **Gemma 3 1B-IT**: A 1 billion parameter instruction-tuned model from Google\n",
        "- Loaded with safetensors format for efficient memory usage\n",
        "- Sharded across TPU cores using the mesh configuration\n",
        "\n",
        "**Note**: You need to have accepted the Gemma license on Kaggle to download the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 7: Download and Load Base Model\n",
        "# ============================================================================\n",
        "\n",
        "# Download model from Hugging Face (skip PyTorch weights)\n",
        "ignore_patterns = [\"*.pth\"]\n",
        "print(f\"Downloading {MODEL_ID} from Hugging Face...\")\n",
        "local_model_path = snapshot_download(\n",
        "    repo_id=MODEL_ID, \n",
        "    ignore_patterns=ignore_patterns\n",
        ")\n",
        "print(f\"Model downloaded to: {local_model_path}\")\n",
        "\n",
        "# Load EOS tokens from generation config\n",
        "EOS_TOKENS = []\n",
        "generation_config_path = os.path.join(local_model_path, \"generation_config.json\")\n",
        "if os.path.exists(generation_config_path):\n",
        "    with open(generation_config_path, \"r\") as f:\n",
        "        generation_configs = json.load(f)\n",
        "    EOS_TOKENS = generation_configs.get(\"eos_token_id\", [])\n",
        "    print(f\"EOS token IDs: {EOS_TOKENS}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 8: Initialize Model with TPU Mesh\n",
        "# ============================================================================\n",
        "\n",
        "# Select model configuration based on MODEL_ID\n",
        "if \"gemma-3-270m\" in MODEL_ID:\n",
        "    model_config = gemma_lib.ModelConfig.gemma3_270m()\n",
        "elif \"gemma-3-1b\" in MODEL_ID:\n",
        "    model_config = gemma_lib.ModelConfig.gemma3_1b_it()\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported model: {MODEL_ID}\")\n",
        "\n",
        "# Create TPU mesh for distributed training\n",
        "mesh = jax.make_mesh(\n",
        "    *MESH, \n",
        "    axis_types=(jax.sharding.AxisType.Auto,) * len(MESH[0])\n",
        ")\n",
        "\n",
        "# Load model with proper sharding\n",
        "print(\"Loading model onto TPU mesh...\")\n",
        "with mesh:\n",
        "    gemma3 = params_safetensors_lib.create_model_from_safe_tensors(\n",
        "        local_model_path, \n",
        "        model_config, \n",
        "        mesh\n",
        "    )\n",
        "\n",
        "print(\"Base model loaded successfully!\")\n",
        "nnx.display(gemma3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LoRA (Low-Rank Adaptation) Setup\n",
        "\n",
        "LoRA is a technique that allows us to fine-tune large models efficiently by:\n",
        "1. Freezing the original model weights\n",
        "2. Adding small trainable \"adapter\" matrices to key layers\n",
        "3. Only training these adapter matrices (much fewer parameters!)\n",
        "\n",
        "**Benefits**:\n",
        "- Reduces memory usage significantly\n",
        "- Faster training\n",
        "- Easy to switch between different fine-tuned versions\n",
        "- Original model weights remain unchanged\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 9: Create LoRA Model and Load Tokenizer\n",
        "# ============================================================================\n",
        "\n",
        "def get_lora_model(base_model, mesh):\n",
        "    \"\"\"\n",
        "    Apply LoRA adapters to the base model.\n",
        "    \n",
        "    LoRA targets specific layers:\n",
        "    - q_einsum, kv_einsum: Attention query/key-value projections\n",
        "    - gate_proj, down_proj, up_proj: MLP layers\n",
        "    - attn_vec_einsum: Attention output projection\n",
        "    \"\"\"\n",
        "    lora_provider = qwix.LoraProvider(\n",
        "        module_path=(\n",
        "            \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|\"\n",
        "            \".*attn_vec_einsum\"\n",
        "        ),\n",
        "        rank=RANK,\n",
        "        alpha=ALPHA,\n",
        "    )\n",
        "    \n",
        "    model_input = base_model.get_model_input()\n",
        "    lora_model = qwix.apply_lora_to_model(\n",
        "        base_model, lora_provider, **model_input\n",
        "    )\n",
        "    \n",
        "    # Shard the LoRA model across devices\n",
        "    with mesh:\n",
        "        state = nnx.state(lora_model)\n",
        "        pspecs = nnx.get_partition_spec(state)\n",
        "        sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
        "        nnx.update(lora_model, sharded_state)\n",
        "    \n",
        "    return lora_model\n",
        "\n",
        "\n",
        "# Create the policy model with LoRA adapters\n",
        "print(\"Creating LoRA policy model...\")\n",
        "lora_policy = get_lora_model(gemma3, mesh=mesh)\n",
        "print(\"LoRA model created!\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = tokenizer_lib.Tokenizer(tokenizer_path=GEMMA_TOKENIZER_PATH)\n",
        "if tokenizer.eos_id() not in EOS_TOKENS:\n",
        "    EOS_TOKENS.append(tokenizer.eos_id())\n",
        "print(f\"Tokenizer loaded. EOS tokens: {EOS_TOKENS}\")\n",
        "\n",
        "# Display memory usage\n",
        "show_hbm_usage()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Data Preparation\n",
        "\n",
        "### Understanding the Two Datasets\n",
        "\n",
        "We use **two different datasets** for the two training stages:\n",
        "\n",
        "---\n",
        "\n",
        "#### Dataset 1: Bespoke-Stratos-17k (for Cold Start SFT)\n",
        "\n",
        "**Source**: `bespokelabs/Bespoke-Stratos-17k` on HuggingFace\n",
        "\n",
        "**Purpose**: Teach the model the reasoning format (`<reasoning>...<answer>`)\n",
        "\n",
        "**Key Features**:\n",
        "- ~17,000 high-quality Chain-of-Thought examples\n",
        "- Distilled from DeepSeek-R1 model\n",
        "- Contains LONG reasoning traces with self-reflection and verification\n",
        "- Includes `<think>...</think>` format (we convert to `<reasoning>...</reasoning>`)\n",
        "- Perfect for cold-start: teaches the model HOW to think, not just WHAT to answer\n",
        "\n",
        "**Why this dataset?**\n",
        "- Shows the model examples of step-by-step reasoning\n",
        "- Teaches proper output structure\n",
        "- Prevents chaotic/unreadable outputs during GRPO\n",
        "\n",
        "---\n",
        "\n",
        "#### Dataset 2: GSM8K / grade-school-math-8k-q-a (for GRPO)\n",
        "\n",
        "**Source**: `thedevastator/grade-school-math-8k-q-a` on Kaggle (or OpenAI's GSM8K)\n",
        "\n",
        "**Purpose**: Strengthen reasoning through reward-based learning\n",
        "\n",
        "**Key Features**:\n",
        "- ~8,000 grade school math word problems\n",
        "- Simple format: question + answer (with `####` separator)\n",
        "- Answers are verifiable (correct or incorrect)\n",
        "- Perfect for GRPO: clear right/wrong signal for rewards\n",
        "\n",
        "**Why this dataset?**\n",
        "- GRPO needs verifiable answers to compute rewards\n",
        "- Math problems have definitive correct answers\n",
        "- Model learns to reason correctly, not just format correctly\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 10: Data Loading Functions\n",
        "# ============================================================================\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# DATASET 1: Bespoke-Stratos-17k for Cold Start SFT\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "def load_bespoke_stratos_dataset(num_examples=None):\n",
        "    \"\"\"\n",
        "    Load Bespoke-Stratos-17k dataset from HuggingFace for Cold Start SFT.\n",
        "    \n",
        "    This dataset contains high-quality Chain-of-Thought examples distilled\n",
        "    from DeepSeek-R1. It's specifically designed for teaching models the\n",
        "    reasoning format.\n",
        "    \n",
        "    Original format uses <think>...</think>, we convert to <reasoning>...</reasoning>\n",
        "    \n",
        "    Args:\n",
        "        num_examples: Maximum number of examples to load (None = all ~17k)\n",
        "    \n",
        "    Returns:\n",
        "        List of dicts with 'prompt' and 'completion' keys\n",
        "    \"\"\"\n",
        "    print(\"Loading Bespoke-Stratos-17k from HuggingFace...\")\n",
        "    \n",
        "    # Load from HuggingFace\n",
        "    dataset = load_dataset(\"bespokelabs/Bespoke-Stratos-17k\", split=\"train\")\n",
        "    \n",
        "    sft_data = []\n",
        "    for i, example in enumerate(tqdm(dataset, desc=\"Processing examples\")):\n",
        "        if num_examples and i >= num_examples:\n",
        "            break\n",
        "        \n",
        "        # Extract conversations\n",
        "        conversations = example.get(\"conversations\", [])\n",
        "        if len(conversations) < 2:\n",
        "            continue\n",
        "        \n",
        "        # Get user message and assistant response\n",
        "        user_msg = None\n",
        "        assistant_msg = None\n",
        "        \n",
        "        for conv in conversations:\n",
        "            role = conv.get(\"from\", conv.get(\"role\", \"\"))\n",
        "            content = conv.get(\"value\", conv.get(\"content\", \"\"))\n",
        "            \n",
        "            if role in [\"human\", \"user\"]:\n",
        "                user_msg = content\n",
        "            elif role in [\"gpt\", \"assistant\"]:\n",
        "                assistant_msg = content\n",
        "        \n",
        "        if not user_msg or not assistant_msg:\n",
        "            continue\n",
        "        \n",
        "        # Convert various thinking tags to our standard <reasoning>...</reasoning> format\n",
        "        # Bespoke-Stratos uses: <|begin_of_thought|>...<|end_of_thought|>\n",
        "        # Some datasets use: <think>...</think>\n",
        "        # Also handle: <think>...</think> or <thinking>...</thinking>\n",
        "        completion = assistant_msg\n",
        "        \n",
        "        # Handle Bespoke-Stratos format\n",
        "        completion = completion.replace(\"<|begin_of_thought|>\", reasoning_start)\n",
        "        completion = completion.replace(\"<|end_of_thought|>\", reasoning_end)\n",
        "        completion = completion.replace(\"<|begin_of_solution|>\", solution_start)\n",
        "        completion = completion.replace(\"<|end_of_solution|>\", solution_end)\n",
        "        \n",
        "        # Handle redacted_reasoning format (convert to reasoning)\n",
        "        completion = completion.replace(\"<think>\", reasoning_start)\n",
        "        completion = completion.replace(\"</think>\", reasoning_end)\n",
        "        \n",
        "        # Handle think/thinking tags (convert to reasoning)\n",
        "        completion = re.sub(r\"<think>\", reasoning_start, completion, flags=re.IGNORECASE)\n",
        "        completion = re.sub(r\"</think>\", reasoning_end, completion, flags=re.IGNORECASE)\n",
        "        completion = re.sub(r\"<thinking>\", reasoning_start, completion, flags=re.IGNORECASE)\n",
        "        completion = re.sub(r\"</thinking>\", reasoning_end, completion, flags=re.IGNORECASE)\n",
        "        \n",
        "        # If still no answer tags, add them\n",
        "        if solution_start not in completion:\n",
        "            # Try to extract final answer after reasoning\n",
        "            if reasoning_end in completion:\n",
        "                parts = completion.split(reasoning_end)\n",
        "                if len(parts) > 1 and parts[1].strip():\n",
        "                    # Wrap the part after reasoning in answer tags\n",
        "                    completion = parts[0] + reasoning_end + \"\\n\" + solution_start + parts[1].strip() + solution_end\n",
        "                else:\n",
        "                    completion = completion + f\"\\n{solution_start}See reasoning above{solution_end}\"\n",
        "            else:\n",
        "                completion = completion + f\"\\n{solution_start}See reasoning above{solution_end}\"\n",
        "        \n",
        "        # Format prompt with our template\n",
        "        prompt = TEMPLATE.format(\n",
        "            system_prompt=SYSTEM_PROMPT,\n",
        "            question=user_msg,\n",
        "        )\n",
        "        \n",
        "        sft_data.append({\n",
        "            \"prompt\": prompt,\n",
        "            \"completion\": completion,\n",
        "        })\n",
        "    \n",
        "    print(f\"Loaded {len(sft_data)} SFT examples from Bespoke-Stratos-17k\")\n",
        "    return sft_data\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# DATASET 2: GSM8K for GRPO Training\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "def download_kaggle_dataset(target_dir=\"./data/gsm8k\"):\n",
        "    \"\"\"Download GSM8K dataset from Kaggle.\"\"\"\n",
        "    os.makedirs(target_dir, exist_ok=True)\n",
        "    src = kagglehub.dataset_download(\"thedevastator/grade-school-math-8k-q-a\")\n",
        "    src = Path(src)\n",
        "    dst = Path(target_dir)\n",
        "    \n",
        "    for csv_file in src.glob(\"*.csv\"):\n",
        "        shutil.copy2(csv_file, dst / csv_file.name)\n",
        "        print(f\"Copied {csv_file.name}\")\n",
        "    return target_dir\n",
        "\n",
        "\n",
        "def get_grpo_dataset(data_dir, split=\"train\", source=\"kaggle\") -> grain.MapDataset:\n",
        "    \"\"\"\n",
        "    Load and format dataset for GRPO training.\n",
        "    \n",
        "    Returns a dataset with:\n",
        "    - prompts: Formatted input prompts\n",
        "    - question: Original question text  \n",
        "    - answer: Ground truth answer for verification\n",
        "    \"\"\"\n",
        "    if source == \"tfds\":\n",
        "        import tensorflow_datasets.text.gsm8k\n",
        "        data = tfds.data_source(\n",
        "            \"gsm8k\",\n",
        "            split=split,\n",
        "            data_dir=data_dir,\n",
        "            builder_kwargs={\"file_format\": tfds.core.FileFormat.ARRAY_RECORD},\n",
        "            download=True,\n",
        "        )\n",
        "    elif source == \"kaggle\":\n",
        "        kaggle_dir = download_kaggle_dataset(data_dir)\n",
        "        file_name = \"main_\" + split + \".csv\"\n",
        "        csv_path = os.path.join(kaggle_dir, file_name)\n",
        "        \n",
        "        data = []\n",
        "        with open(csv_path, newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "            reader = csv.DictReader(csvfile)\n",
        "            for row in reader:\n",
        "                data.append({\n",
        "                    \"question\": row[\"question\"],\n",
        "                    \"answer\": row[\"answer\"],\n",
        "                })\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown source: {source}\")\n",
        "    \n",
        "    def _as_text(v):\n",
        "        return v if isinstance(v, str) else v.decode(\"utf-8\")\n",
        "    \n",
        "    dataset = (\n",
        "        grain.MapDataset.source(data)\n",
        "        .shuffle(seed=42)\n",
        "        .map(lambda x: {\n",
        "            \"prompts\": TEMPLATE.format(\n",
        "                system_prompt=SYSTEM_PROMPT,\n",
        "                question=_as_text(x[\"question\"]),\n",
        "            ),\n",
        "            \"question\": _as_text(x[\"question\"]),\n",
        "            \"answer\": extract_hash_answer(_as_text(x[\"answer\"])),\n",
        "        })\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "\n",
        "print(\"Data loading functions defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 10.5: Load Evaluation Dataset and Define Evaluation Functions\n",
        "# ============================================================================\n",
        "# Load a small test dataset for evaluation before and after SFT training.\n",
        "# Also define evaluate and generate functions here for use in comparison cells.\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"PREPARING EVALUATION DATASET\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load a small test dataset for evaluation (before and after SFT)\n",
        "NUM_EVAL_SAMPLES = 32  # Small sample for quick evaluation\n",
        "eval_test_dataset = get_grpo_dataset(TEST_DATA_DIR, \"test\", \"kaggle\")\n",
        "eval_test_dataset = eval_test_dataset.batch(1)[:NUM_EVAL_SAMPLES]\n",
        "\n",
        "print(f\"Loaded {NUM_EVAL_SAMPLES} test samples for evaluation\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Define evaluation functions (same as in original notebook)\n",
        "def generate(question, sampler, temperature=0.7, top_k=50, top_p=0.95, seed=None):\n",
        "    \"\"\"Generate response for a given question.\"\"\"\n",
        "    if isinstance(question, str):\n",
        "        input_batch = [TEMPLATE.format(system_prompt=SYSTEM_PROMPT, question=question)]\n",
        "    else:\n",
        "        input_batch = [\n",
        "            TEMPLATE.format(system_prompt=SYSTEM_PROMPT, question=q)\n",
        "            for q in question\n",
        "        ]\n",
        "    \n",
        "    out_data = sampler(\n",
        "        input_strings=input_batch,\n",
        "        max_generation_steps=TOTAL_GENERATION_STEPS,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        echo=False,\n",
        "        seed=seed,\n",
        "        eos_tokens=EOS_TOKENS,\n",
        "    )\n",
        "    \n",
        "    output = out_data.text\n",
        "    return output[0] if isinstance(question, str) else output\n",
        "\n",
        "\n",
        "def evaluate(dataset, sampler, temperature=0.7, top_k=50, top_p=0.95, debug_samples=3):\n",
        "    \"\"\"Evaluate model on dataset and compute metrics.\"\"\"\n",
        "    correct = 0\n",
        "    partially_correct = 0\n",
        "    correct_format = 0\n",
        "    total = 0\n",
        "    \n",
        "    for batch_idx, batch in enumerate(tqdm(dataset, desc=\"Evaluating\")):\n",
        "        answers = batch[\"answer\"]\n",
        "        questions = batch[\"question\"]\n",
        "        \n",
        "        responses = generate(questions, sampler, temperature, top_k, top_p)\n",
        "        \n",
        "        for q_idx, (question, response, answer) in enumerate(zip(questions, responses, answers)):\n",
        "            # Debug: Print first few samples to see what model generates\n",
        "            if total < debug_samples:\n",
        "                print(f\"\\n--- Sample {total + 1} ---\")\n",
        "                print(f\"Question: {question[:100]}...\")\n",
        "                print(f\"Expected Answer: {answer}\")\n",
        "                print(f\"Model Response (first 500 chars):\\n{response[:500]}\")\n",
        "                print(f\"Full Response Length: {len(response)} chars\")\n",
        "                # Check if format matches\n",
        "                format_match = match_format.search(response)\n",
        "                print(f\"Format Match: {format_match is not None}\")\n",
        "                if format_match:\n",
        "                    print(f\"Matched Format: {format_match.group(0)[:200]}...\")\n",
        "                # Check if numbers can be extracted\n",
        "                num_match = match_numbers.search(response)\n",
        "                print(f\"Number Match: {num_match is not None}\")\n",
        "                if num_match:\n",
        "                    print(f\"Extracted Number: {num_match.group(1)}\")\n",
        "                print(\"-\" * 60)\n",
        "            \n",
        "            # Check answer\n",
        "            extracted = match_numbers.search(response)\n",
        "            if extracted:\n",
        "                try:\n",
        "                    pred = float(extracted.group(1).strip())\n",
        "                    true_val = float(answer.strip())\n",
        "                    if pred == true_val:\n",
        "                        correct += 1\n",
        "                    ratio = pred / true_val if true_val != 0 else 0\n",
        "                    if 0.9 <= ratio <= 1.1:\n",
        "                        partially_correct += 1\n",
        "                except:\n",
        "                    pass\n",
        "            \n",
        "            # Check format\n",
        "            if match_format.search(response):\n",
        "                correct_format += 1\n",
        "            \n",
        "            total += 1\n",
        "            \n",
        "            # Print progress\n",
        "            if total % 10 == 0:\n",
        "                print(f\"Progress: {correct}/{total} correct ({correct/total*100:.1f}%)\")\n",
        "    \n",
        "    return {\n",
        "        \"accuracy\": correct / total * 100 if total > 0 else 0,\n",
        "        \"partial_accuracy\": partially_correct / total * 100 if total > 0 else 0,\n",
        "        \"format_accuracy\": correct_format / total * 100 if total > 0 else 0,\n",
        "        \"correct\": correct,\n",
        "        \"total\": total,\n",
        "    }\n",
        "\n",
        "print(\"\\nEvaluation functions defined!\")\n",
        "print(\"Ready for pre-SFT and post-SFT evaluation.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 11.5: Evaluate Model Before SFT Training\n",
        "# ============================================================================\n",
        "# This cell evaluates the model's performance BEFORE cold start SFT training.\n",
        "# Uses the evaluate function to get quantitative metrics.\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"EVALUATING MODEL BEFORE SFT TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nRunning evaluation on test dataset...\")\n",
        "print(\"This will measure accuracy and format compliance before training.\\n\")\n",
        "\n",
        "# Create a sampler for inference\n",
        "pre_sft_sampler = sampler_lib.Sampler(\n",
        "    transformer=lora_policy,\n",
        "    tokenizer=tokenizer,\n",
        "    cache_config=sampler_lib.CacheConfig(\n",
        "        cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
        "        num_layers=model_config.num_layers,\n",
        "        num_kv_heads=model_config.num_kv_heads,\n",
        "        head_dim=model_config.head_dim,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Run evaluation\n",
        "pre_sft_results = evaluate(eval_test_dataset, pre_sft_sampler, **GENERATION_CONFIGS[\"greedy\"])\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PRE-SFT EVALUATION RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"  Answer Accuracy:    {pre_sft_results['accuracy']:.2f}%\")\n",
        "print(f\"  Partial Accuracy:   {pre_sft_results['partial_accuracy']:.2f}%\")\n",
        "print(f\"  Format Accuracy:    {pre_sft_results['format_accuracy']:.2f}%\")\n",
        "print(f\"  Correct/Total:      {pre_sft_results['correct']}/{pre_sft_results['total']}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"READY FOR SFT TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "print(\"After SFT training, we expect improvements in:\")\n",
        "print(\"  - Format Accuracy: Model should learn <reasoning>...</reasoning><answer>...</answer> format\")\n",
        "print(\"  - Answer Accuracy: Better reasoning may lead to more correct answers\")\n",
        "print(\"=\" * 60 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Stage 1 - Cold Start SFT Training\n",
        "\n",
        "Cold Start SFT teaches the model the output format before GRPO strengthens reasoning.\n",
        "\n",
        "**Key Difference from Original**: This notebook uses `PeftTrainer` from Tunix (following qlora_gemma approach) instead of a custom training loop.\n",
        "\n",
        "Key goals:\n",
        "- Teach the `<reasoning>` and `<answer>` tag format\n",
        "- Show examples of step-by-step thinking\n",
        "- Prevent chaotic outputs during later RL training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 11: Prepare SFT Dataset (Bespoke-Stratos-17k)\n",
        "# ============================================================================\n",
        "# We use Bespoke-Stratos-17k for cold start SFT - this dataset contains\n",
        "# high-quality Chain-of-Thought examples that teach proper reasoning format.\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Loading SFT Dataset: Bespoke-Stratos-17k\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load Bespoke-Stratos-17k from HuggingFace\n",
        "# Set num_examples to limit for faster training (None = use all ~17k)\n",
        "SFT_NUM_EXAMPLES = 2000  # Adjust based on available time\n",
        "\n",
        "sft_data = load_bespoke_stratos_dataset(num_examples=SFT_NUM_EXAMPLES)\n",
        "\n",
        "# Preview an example\n",
        "print(\"\\nSample SFT training example:\")\n",
        "print(\"=\" * 60)\n",
        "sample = sft_data[0]\n",
        "print(f\"PROMPT:\\n{sample['prompt'][:200]}...\")\n",
        "print(f\"\\nCOMPLETION:\\n{sample['completion'][:300]}...\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 12: Create SFT Dataset in Grain Format for PeftTrainer\n",
        "# ============================================================================\n",
        "# PeftTrainer expects a dataset that yields TrainingInput objects.\n",
        "# We need to convert our prompt+completion pairs into the right format.\n",
        "\n",
        "def create_sft_training_dataset(sft_data, tokenizer, max_length=MAX_SEQ_LENGTH):\n",
        "    \"\"\"\n",
        "    Create a Grain dataset for SFT training that works with PeftTrainer.\n",
        "    \n",
        "    This function:\n",
        "    1. Combines prompt and completion into full text\n",
        "    2. Tokenizes the text\n",
        "    3. Creates TrainingInput objects that PeftTrainer expects\n",
        "    \"\"\"\n",
        "    def format_example(example):\n",
        "        \"\"\"Combine prompt and completion into a single training text.\"\"\"\n",
        "        full_text = example[\"prompt\"] + example[\"completion\"]\n",
        "        return {\"text\": full_text}\n",
        "    \n",
        "    def tokenize_and_pad_example(example):\n",
        "        \"\"\"Tokenize the text and pad to fixed length.\"\"\"\n",
        "        text = example[\"text\"]\n",
        "        tokens = tokenizer.encode(text)\n",
        "        \n",
        "        # Track original length before truncation/padding\n",
        "        original_length = len(tokens)\n",
        "        \n",
        "        # Truncate if too long\n",
        "        if len(tokens) > max_length:\n",
        "            tokens = tokens[:max_length]\n",
        "            actual_length = max_length\n",
        "        else:\n",
        "            actual_length = original_length\n",
        "        \n",
        "        # Pad to fixed length max_length\n",
        "        pad_id = tokenizer.pad_id()\n",
        "        if len(tokens) < max_length:\n",
        "            padding = [pad_id] * (max_length - len(tokens))\n",
        "            tokens = tokens + padding\n",
        "        \n",
        "        # Create input_tokens (padded to max_length)\n",
        "        input_tokens = np.array(tokens, dtype=np.int32)\n",
        "        \n",
        "        # Create input_mask (1 for valid tokens, 0 for padding)\n",
        "        input_mask = np.ones((max_length,), dtype=np.float32)\n",
        "        input_mask[actual_length:] = 0.0\n",
        "        \n",
        "        return {\n",
        "            \"input_tokens\": input_tokens,\n",
        "            \"input_mask\": input_mask,\n",
        "        }\n",
        "    \n",
        "    # Create dataset pipeline\n",
        "    dataset = (\n",
        "        grain.MapDataset.source(sft_data)\n",
        "        .shuffle(seed=42)\n",
        "        .map(format_example)\n",
        "        .map(tokenize_and_pad_example)\n",
        "        .batch(SFT_BATCH_SIZE)\n",
        "    )\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Create training and validation datasets\n",
        "print(\"Creating SFT training dataset...\")\n",
        "sft_dataset = create_sft_training_dataset(sft_data, tokenizer, max_length=MAX_SEQ_LENGTH)\n",
        "\n",
        "# Split into train/validation (80/20 split)\n",
        "total_examples = len(sft_data)\n",
        "train_size = int(total_examples * 0.8)\n",
        "val_size = total_examples - train_size\n",
        "\n",
        "print(f\"Total examples: {total_examples}\")\n",
        "print(f\"Train examples: {train_size}\")\n",
        "print(f\"Validation examples: {val_size}\")\n",
        "\n",
        "# Note: For simplicity, we'll use the full dataset for training\n",
        "# In production, you might want to split properly\n",
        "train_ds = sft_dataset\n",
        "validation_ds = sft_dataset  # Use same for validation (or split properly)\n",
        "\n",
        "print(\"SFT dataset created successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 13: Define Model Input Function for PeftTrainer\n",
        "# ============================================================================\n",
        "# PeftTrainer requires a function that converts TrainingInput to model inputs.\n",
        "# This follows the qlora_gemma approach.\n",
        "# Note: We handle both TrainingInput objects and dict inputs for flexibility.\n",
        "\n",
        "def gen_model_input_fn(x):\n",
        "    \"\"\"\n",
        "    Convert TrainingInput (or dict) to model input format.\n",
        "    \n",
        "    This function:\n",
        "    1. Handles both TrainingInput objects and dict inputs\n",
        "    2. Creates position indices from padding mask\n",
        "    3. Creates causal attention mask\n",
        "    4. Returns dict with all required model inputs\n",
        "    \"\"\"\n",
        "    # Handle both TrainingInput objects and dict inputs\n",
        "    if isinstance(x, dict):\n",
        "        input_tokens = x['input_tokens']\n",
        "        input_mask = x['input_mask']\n",
        "    else:\n",
        "        # TrainingInput object\n",
        "        input_tokens = x.input_tokens\n",
        "        input_mask = x.input_mask\n",
        "    \n",
        "    pad_mask = input_tokens != tokenizer.pad_id()\n",
        "    positions = utils.build_positions_from_mask(pad_mask)\n",
        "    attention_mask = utils.make_causal_attn_mask(pad_mask)\n",
        "    return {\n",
        "        'input_tokens': input_tokens,\n",
        "        'input_mask': input_mask,\n",
        "        'positions': positions,\n",
        "        'attention_mask': attention_mask,\n",
        "    }\n",
        "\n",
        "print(\"Model input function defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 13.6: Launch TensorBoard for Monitoring\n",
        "# ============================================================================\n",
        "\n",
        "# Load TensorBoard extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Start TensorBoard to monitor training metrics\n",
        "%tensorboard --logdir ./logs/ --port=0\n",
        "\n",
        "print(\"TensorBoard launched! Monitor training metrics above.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 14: Run Cold Start SFT Training using PeftTrainer\n",
        "# ============================================================================\n",
        "# This stage teaches the model the reasoning format using Bespoke-Stratos-17k.\n",
        "# We use PeftTrainer from Tunix (following qlora_gemma approach) instead of\n",
        "# a custom training loop.\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"STAGE 1: COLD START SFT TRAINING (using PeftTrainer)\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nUsing Bespoke-Stratos-17k dataset to teach reasoning format.\")\n",
        "print(\"This stage plants the <reasoning>...</reasoning><answer>...</answer> template.\")\n",
        "print(\"\\nFollowing qlora_gemma approach with PeftTrainer.\\n\")\n",
        "\n",
        "# Configure SFT optimizer\n",
        "sft_optimizer = optax.adamw(\n",
        "    learning_rate=SFT_LEARNING_RATE,\n",
        "    b1=B1,\n",
        "    b2=B2,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        ")\n",
        "\n",
        "# Add gradient clipping\n",
        "sft_optimizer = optax.chain(\n",
        "    optax.clip_by_global_norm(max_norm=MAX_GRAD_NORM),\n",
        "    sft_optimizer,\n",
        ")\n",
        "\n",
        "# Convert checkpoint directory to absolute path (required by Orbax)\n",
        "SFT_CKPT_DIR_ABS = os.path.abspath(SFT_CKPT_DIR)\n",
        "os.makedirs(SFT_CKPT_DIR_ABS, exist_ok=True)\n",
        "\n",
        "# Configure metrics logging\n",
        "sft_logging_options = metrics_logger.MetricsLoggerOptions(\n",
        "    log_dir=\"./logs/sft/\",\n",
        "    flush_every_n_steps=20,\n",
        ")\n",
        "\n",
        "# Configure training\n",
        "training_config = peft_trainer.TrainingConfig(\n",
        "    eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
        "    max_steps=SFT_MAX_STEPS,\n",
        "    metrics_logging_options=sft_logging_options,\n",
        "    checkpoint_root_directory=SFT_CKPT_DIR_ABS,\n",
        ")\n",
        "\n",
        "# Create PeftTrainer (following qlora_gemma approach)\n",
        "trainer = peft_trainer.PeftTrainer(\n",
        "    lora_policy,  # Use LoRA model for training\n",
        "    sft_optimizer,\n",
        "    training_config\n",
        ").with_gen_model_input_fn(gen_model_input_fn)\n",
        "\n",
        "print(f\"\\nSFT Configuration:\")\n",
        "print(f\"  Learning rate: {SFT_LEARNING_RATE}\")\n",
        "print(f\"  Max steps: {SFT_MAX_STEPS}\")\n",
        "print(f\"  Batch size: {SFT_BATCH_SIZE}\")\n",
        "print(f\"  Checkpoint dir: {SFT_CKPT_DIR_ABS}\")\n",
        "print(f\"  Log dir: ./logs/sft/\")\n",
        "\n",
        "# Run training\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Starting SFT training for {SFT_MAX_STEPS} steps...\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "# The first couple of training steps might take up to 5 minutes to finish.\n",
        "# Please be patient. If you experience long training steps, e.g. >10 minutes\n",
        "# per step, please open a bug. Really appreciated!\n",
        "with mesh:\n",
        "    trainer.train(train_ds, validation_ds)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"COLD START SFT TRAINING COMPLETE!\")\n",
        "print(f\"Checkpoints saved to: {SFT_CKPT_DIR_ABS}\")\n",
        "print(f\"Logs saved to: ./logs/sft/\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "show_hbm_usage()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 14.5: Evaluate Model After SFT Training\n",
        "# ============================================================================\n",
        "# This cell evaluates the model's performance AFTER cold start SFT training.\n",
        "# Uses the evaluate function to show quantitative improvements.\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"EVALUATING MODEL AFTER SFT TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load the latest checkpoint to ensure we're using the trained model\n",
        "# PeftTrainer should update model in-place, but loading from checkpoint ensures correctness\n",
        "import glob\n",
        "\n",
        "# Find the latest checkpoint\n",
        "checkpoint_dirs = glob.glob(os.path.join(SFT_CKPT_DIR_ABS, \"*\"))\n",
        "checkpoint_steps = []\n",
        "for ckpt_dir in checkpoint_dirs:\n",
        "    if os.path.isdir(ckpt_dir):\n",
        "        try:\n",
        "            step = int(os.path.basename(ckpt_dir))\n",
        "            checkpoint_steps.append(step)\n",
        "        except ValueError:\n",
        "            continue\n",
        "\n",
        "if checkpoint_steps:\n",
        "    latest_step = max(checkpoint_steps)\n",
        "    print(f\"\\nLoading checkpoint from step {latest_step}...\")\n",
        "    \n",
        "    # Load checkpoint\n",
        "    ckpt_path = os.path.join(SFT_CKPT_DIR_ABS, str(latest_step), \"model_params\")\n",
        "    \n",
        "    # Get the shape/dtype structure for restoration\n",
        "    abs_params = jax.tree.map(\n",
        "        lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n",
        "        nnx.state(lora_policy, nnx.LoRAParam),\n",
        "    )\n",
        "    \n",
        "    # Restore checkpoint\n",
        "    checkpointer = ocp.StandardCheckpointer()\n",
        "    trained_lora_params = checkpointer.restore(ckpt_path, target=abs_params)\n",
        "    \n",
        "    # Update model with trained parameters\n",
        "    nnx.update(\n",
        "        lora_policy,\n",
        "        jax.tree.map(\n",
        "            lambda a, b: b,\n",
        "            nnx.state(lora_policy, nnx.LoRAParam),\n",
        "            trained_lora_params,\n",
        "        ),\n",
        "    )\n",
        "    \n",
        "    print(f\"Checkpoint loaded successfully from: {ckpt_path}\")\n",
        "else:\n",
        "    print(\"\\nWARNING: No checkpoint found. Using current model state.\")\n",
        "    print(\"Note: PeftTrainer should have updated model in-place during training.\")\n",
        "\n",
        "print(\"\\nRunning evaluation on the same test dataset...\")\n",
        "print(\"This will show quantitative improvements after training.\\n\")\n",
        "\n",
        "# Create sampler for post-SFT inference\n",
        "post_sft_sampler = sampler_lib.Sampler(\n",
        "    transformer=lora_policy,\n",
        "    tokenizer=tokenizer,\n",
        "    cache_config=sampler_lib.CacheConfig(\n",
        "        cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
        "        num_layers=model_config.num_layers,\n",
        "        num_kv_heads=model_config.num_kv_heads,\n",
        "        head_dim=model_config.head_dim,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Run evaluation on the same dataset\n",
        "post_sft_results = evaluate(eval_test_dataset, post_sft_sampler, **GENERATION_CONFIGS[\"greedy\"])\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"POST-SFT EVALUATION RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"  Answer Accuracy:    {post_sft_results['accuracy']:.2f}%\")\n",
        "print(f\"  Partial Accuracy:   {post_sft_results['partial_accuracy']:.2f}%\")\n",
        "print(f\"  Format Accuracy:    {post_sft_results['format_accuracy']:.2f}%\")\n",
        "print(f\"  Correct/Total:      {post_sft_results['correct']}/{post_sft_results['total']}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"BEFORE vs AFTER SFT COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"{'Metric':<20} {'Before SFT':<15} {'After SFT':<15} {'Change':<15}\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "acc_change = post_sft_results['accuracy'] - pre_sft_results['accuracy']\n",
        "partial_change = post_sft_results['partial_accuracy'] - pre_sft_results['partial_accuracy']\n",
        "format_change = post_sft_results['format_accuracy'] - pre_sft_results['format_accuracy']\n",
        "\n",
        "print(f\"{'Answer Accuracy':<20} {pre_sft_results['accuracy']:>6.2f}%      {post_sft_results['accuracy']:>6.2f}%      {acc_change:>+6.2f}%\")\n",
        "print(f\"{'Partial Accuracy':<20} {pre_sft_results['partial_accuracy']:>6.2f}%      {post_sft_results['partial_accuracy']:>6.2f}%      {partial_change:>+6.2f}%\")\n",
        "print(f\"{'Format Accuracy':<20} {pre_sft_results['format_accuracy']:>6.2f}%      {post_sft_results['format_accuracy']:>6.2f}%      {format_change:>+6.2f}%\")\n",
        "print(\"=\" * 65)\n",
        "\n",
        "print(\"\\nKey Improvements:\")\n",
        "if format_change > 0:\n",
        "    print(f\"  ✓ Format accuracy improved by {format_change:.2f}%\")\n",
        "if acc_change > 0:\n",
        "    print(f\"  ✓ Answer accuracy improved by {acc_change:.2f}%\")\n",
        "if acc_change <= 0 and format_change <= 0:\n",
        "    print(\"  Note: Model is learning the format, accuracy improvements may come in GRPO stage\")\n",
        "print(\"=\" * 60 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 7: Stage 2 - GRPO Training\n",
        "\n",
        "### What is GRPO?\n",
        "\n",
        "GRPO (Group Relative Policy Optimization) is a reinforcement learning algorithm that:\n",
        "1. Generates multiple responses for each prompt\n",
        "2. Scores each response using reward functions\n",
        "3. Updates the model to favor higher-scoring responses\n",
        "\n",
        "Key advantages over PPO:\n",
        "- No need for a separate value/critic model (saves memory!)\n",
        "- Uses group-based advantage estimation\n",
        "- More stable training for reasoning tasks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 15: Define Reward Functions for GRPO\n",
        "# ============================================================================\n",
        "# These functions evaluate model outputs and provide learning signals.\n",
        "\n",
        "def match_format_exactly(prompts, completions, **kwargs):\n",
        "    \"\"\"\n",
        "    Reward function 1: Exact format matching\n",
        "    \n",
        "    Awards 3 points if the output exactly matches the expected format:\n",
        "    <reasoning>...</reasoning><answer>...</answer>\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "    for response in completions:\n",
        "        match = match_format.search(response)\n",
        "        rewards.append(3.0 if match else 0.0)\n",
        "    return rewards\n",
        "\n",
        "\n",
        "def match_format_approximately(prompts, completions, **kwargs):\n",
        "    \"\"\"\n",
        "    Reward function 2: Partial format matching\n",
        "    \n",
        "    Awards partial credit for having some of the required elements.\n",
        "    This helps the model learn incrementally.\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "    for completion in completions:\n",
        "        score = 0\n",
        "        # Check for each required element\n",
        "        score += 0.5 if completion.count(reasoning_start) == 1 else -0.5\n",
        "        score += 0.5 if completion.find(reasoning_start) == 0 else -0.5\n",
        "        score += 0.5 if completion.count(reasoning_end) == 1 else -0.5\n",
        "        score += 0.5 if completion.count(solution_start) == 1 else -0.5\n",
        "        score += 0.5 if completion.count(solution_end) == 1 else -0.5\n",
        "        scores.append(score)\n",
        "    return scores\n",
        "\n",
        "\n",
        "def check_answer(prompts, completions, answer, **kwargs):\n",
        "    \"\"\"\n",
        "    Reward function 3: Answer correctness\n",
        "    \n",
        "    Awards points based on how close the predicted answer is to the truth:\n",
        "    - 3.0 points for exact match\n",
        "    - 1.5 points for match after stripping whitespace\n",
        "    - 0.5 points if within 10% of correct value\n",
        "    - Penalties for wrong answers\n",
        "    \"\"\"\n",
        "    extracted_responses = [\n",
        "        guess.group(1) if r is not None and (guess := match_format.search(r)) else None\n",
        "        for r in completions\n",
        "    ]\n",
        "    \n",
        "    scores = []\n",
        "    for guess, true_answer in zip(extracted_responses, answer):\n",
        "        score = 0\n",
        "        if guess is None:\n",
        "            scores.append(0)\n",
        "            continue\n",
        "            \n",
        "        if guess == true_answer:\n",
        "            score += 3.0\n",
        "        elif guess.strip() == true_answer.strip():\n",
        "            score += 1.5\n",
        "        else:\n",
        "            try:\n",
        "                ratio = float(guess) / float(true_answer)\n",
        "                if 0.9 <= ratio <= 1.1:\n",
        "                    score += 0.5\n",
        "                elif 0.8 <= ratio <= 1.2:\n",
        "                    score += 0.25\n",
        "                else:\n",
        "                    score -= 1.0\n",
        "            except:\n",
        "                score -= 0.5\n",
        "        scores.append(score)\n",
        "    return scores\n",
        "\n",
        "\n",
        "def check_numbers(prompts, completions, answer, **kwargs):\n",
        "    \"\"\"\n",
        "    Reward function 4: Numerical answer extraction\n",
        "    \n",
        "    Sometimes the answer tag contains extra text. This function\n",
        "    extracts numerical values and compares them.\n",
        "    \"\"\"\n",
        "    extracted_responses = [\n",
        "        guess.group(1) if (guess := match_numbers.search(r)) else None\n",
        "        for r in completions\n",
        "    ]\n",
        "    \n",
        "    scores = []\n",
        "    for guess, true_answer in zip(extracted_responses, answer):\n",
        "        if guess is None:\n",
        "            scores.append(0)\n",
        "            continue\n",
        "        try:\n",
        "            true_val = float(true_answer.strip())\n",
        "            pred_val = float(guess.strip())\n",
        "            scores.append(1.5 if pred_val == true_val else 0.0)\n",
        "        except:\n",
        "            scores.append(0)\n",
        "    return scores\n",
        "\n",
        "\n",
        "print(\"Reward functions defined:\")\n",
        "print(\"  1. match_format_exactly: Rewards correct output structure\")\n",
        "print(\"  2. match_format_approximately: Partial credit for format\")\n",
        "print(\"  3. check_answer: Rewards correct answers\")\n",
        "print(\"  4. check_numbers: Extracts and compares numerical answers\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 16: Prepare GRPO Dataset\n",
        "# ============================================================================\n",
        "\n",
        "print(\"Loading GRPO training data...\")\n",
        "\n",
        "# Load GSM8K dataset for GRPO training\n",
        "NUM_GRPO_BATCHES = 3738  # Full dataset\n",
        "if DEBUG_MODE:\n",
        "    NUM_GRPO_BATCHES = 10  # Reduced for fast debugging (use only 10 batches)\n",
        "NUM_TEST_BATCHES = 64    # For evaluation\n",
        "if DEBUG_MODE:\n",
        "    NUM_TEST_BATCHES = 5  # Reduced for fast debugging\n",
        "\n",
        "grpo_dataset = get_grpo_dataset(TRAIN_DATA_DIR, \"train\", \"kaggle\")\n",
        "grpo_dataset = grpo_dataset.batch(GRPO_BATCH_SIZE)[:NUM_GRPO_BATCHES]\n",
        "\n",
        "# Split into train/validation\n",
        "if TRAIN_FRACTION < 1.0:\n",
        "    split_idx = int(len(grpo_dataset) * TRAIN_FRACTION)\n",
        "    train_dataset = grpo_dataset[:split_idx]\n",
        "    val_dataset = grpo_dataset[split_idx:]\n",
        "else:\n",
        "    train_dataset = grpo_dataset\n",
        "    val_dataset = None\n",
        "\n",
        "# Load test dataset\n",
        "test_dataset = get_grpo_dataset(TEST_DATA_DIR, \"test\", \"kaggle\")\n",
        "test_dataset = test_dataset.batch(GRPO_BATCH_SIZE)[:NUM_TEST_BATCHES]\n",
        "\n",
        "print(f\"\\nDataset sizes:\")\n",
        "print(f\"  Training batches: {len(train_dataset)}\")\n",
        "print(f\"  Validation batches: {len(val_dataset) if val_dataset else 0}\")\n",
        "print(f\"  Test batches: {len(test_dataset)}\")\n",
        "\n",
        "# Preview a sample\n",
        "print(\"\\nSample GRPO training example:\")\n",
        "for batch in train_dataset[:1]:\n",
        "    pprint(batch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 17: Configure and Run GRPO Training\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"STAGE 2: GRPO REINFORCEMENT LEARNING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Calculate training steps\n",
        "GRPO_WARMUP_STEPS = int(GRPO_WARMUP_RATIO * GRPO_MAX_STEPS)\n",
        "EVAL_EVERY_N_STEPS = 64\n",
        "if DEBUG_MODE:\n",
        "    EVAL_EVERY_N_STEPS = 5  # More frequent evaluation for debugging\n",
        "\n",
        "# Configure GRPO optimizer (lower learning rate than SFT!)\n",
        "grpo_optimizer = optax.adamw(\n",
        "    learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n",
        "        init_value=0.0,\n",
        "        peak_value=GRPO_LEARNING_RATE,\n",
        "        warmup_steps=GRPO_WARMUP_STEPS,\n",
        "        decay_steps=GRPO_MAX_STEPS,\n",
        "        end_value=0.0,\n",
        "    ),\n",
        "    b1=B1,\n",
        "    b2=B2,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        ")\n",
        "\n",
        "# Add gradient clipping (critical for RL stability!)\n",
        "grpo_optimizer = optax.chain(\n",
        "    optax.clip_by_global_norm(max_norm=MAX_GRAD_NORM),\n",
        "    grpo_optimizer,\n",
        ")\n",
        "\n",
        "# Convert GRPO checkpoint directory to absolute path (required by Orbax)\n",
        "GRPO_CKPT_DIR_ABS = os.path.abspath(GRPO_CKPT_DIR)\n",
        "os.makedirs(GRPO_CKPT_DIR_ABS, exist_ok=True)\n",
        "\n",
        "# Checkpoint configuration\n",
        "grpo_ckpt_options = ocp.CheckpointManagerOptions(\n",
        "    save_interval_steps=SAVE_INTERVAL_STEPS,\n",
        "    max_to_keep=MAX_TO_KEEP,\n",
        ")\n",
        "\n",
        "# Metrics logging\n",
        "grpo_metrics_options = metrics_logger.MetricsLoggerOptions(\n",
        "    log_dir=\"./logs/grpo/\",\n",
        "    flush_every_n_steps=20,\n",
        ")\n",
        "\n",
        "print(f\"\\nGRPO Configuration:\")\n",
        "print(f\"  Learning rate: {GRPO_LEARNING_RATE}\")\n",
        "print(f\"  Generations per prompt (G): {NUM_GENERATIONS}\")\n",
        "print(f\"  KL penalty (beta): {BETA}\")\n",
        "print(f\"  Clipping (epsilon): {EPSILON}\")\n",
        "print(f\"  Max steps: {GRPO_MAX_STEPS}\")\n",
        "\n",
        "# Training configuration\n",
        "cluster_config = rl_cluster_lib.ClusterConfig(\n",
        "    role_to_mesh={\n",
        "        rl_cluster_lib.Role.ACTOR: mesh,\n",
        "        rl_cluster_lib.Role.REFERENCE: mesh,\n",
        "        rl_cluster_lib.Role.ROLLOUT: mesh,\n",
        "    },\n",
        "    rollout_engine='vanilla',\n",
        "    offload_to_cpu=False,\n",
        "    training_config=rl_cluster_lib.RLTrainingConfig(\n",
        "        actor_optimizer=grpo_optimizer,\n",
        "        eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
        "        max_steps=GRPO_MAX_STEPS,\n",
        "        mini_batch_size=GRPO_BATCH_SIZE,\n",
        "        train_micro_batch_size=GRPO_BATCH_SIZE,\n",
        "        metrics_logging_options=grpo_metrics_options,\n",
        "        checkpoint_root_directory=GRPO_CKPT_DIR_ABS,\n",
        "        checkpointing_options=grpo_ckpt_options,\n",
        "    ),\n",
        "    rollout_config=base_rollout.RolloutConfig(\n",
        "        max_tokens_to_generate=TOTAL_GENERATION_STEPS,\n",
        "        max_prompt_length=MAX_PROMPT_LENGTH,\n",
        "        kv_cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
        "        temperature=TEMPERATURE,\n",
        "        top_p=TOP_P,\n",
        "        top_k=TOP_K,\n",
        "        eos_tokens=EOS_TOKENS,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# GRPO algorithm configuration\n",
        "grpo_config = GRPOConfig(\n",
        "    num_generations=NUM_GENERATIONS,\n",
        "    num_iterations=NUM_ITERATIONS,\n",
        "    beta=BETA,\n",
        "    epsilon=EPSILON,\n",
        ")\n",
        "\n",
        "print(\"\\nInitializing RL Cluster...\")\n",
        "# Create RL cluster with policy and reference models\n",
        "rl_cluster = rl_cluster_lib.RLCluster(\n",
        "    actor=lora_policy,      # Model being trained (with LoRA)\n",
        "    reference=gemma3,        # Fixed reference model (for KL divergence)\n",
        "    tokenizer=tokenizer,\n",
        "    cluster_config=cluster_config,\n",
        ")\n",
        "\n",
        "print(\"Creating GRPO Trainer...\")\n",
        "# Create GRPO trainer with reward functions\n",
        "grpo_trainer = GRPOLearner(\n",
        "    rl_cluster=rl_cluster,\n",
        "    reward_fns=[\n",
        "        match_format_exactly,\n",
        "        match_format_approximately,\n",
        "        check_answer,\n",
        "        check_numbers,\n",
        "    ],\n",
        "    algo_config=grpo_config,\n",
        ")\n",
        "\n",
        "print(\"GRPO Trainer initialized successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 18: Run GRPO Training\n",
        "# ============================================================================\n",
        "# This is the main training loop. It may take several hours to complete.\n",
        "# Checkpoints are saved automatically during training.\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STARTING GRPO TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nThis will take approximately 6-8 hours on TPU v6e-1\")\n",
        "print(f\"Checkpoints saved every {SAVE_INTERVAL_STEPS} steps to {GRPO_CKPT_DIR_ABS}\")\n",
        "print(\"\\nMonitor progress in TensorBoard.\")\n",
        "print(\"Key metrics to watch:\")\n",
        "print(\"  - reward_accuracy: Should increase over time\")\n",
        "print(\"  - reward_format: Should stay high (near 1.0)\")\n",
        "print(\"  - completion_length: May increase as model reasons more\")\n",
        "print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "# Run GRPO training\n",
        "grpo_trainer.train(train_dataset, val_dataset)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"GRPO TRAINING COMPLETE!\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 18.5: Evaluate Model After GRPO Training\n",
        "# ============================================================================\n",
        "# This cell evaluates the model's performance AFTER GRPO training.\n",
        "# Shows the final improvements from the complete training pipeline.\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"EVALUATING MODEL AFTER GRPO TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nRunning evaluation on test dataset...\")\n",
        "print(\"This will show the final improvements from the complete training pipeline.\\n\")\n",
        "\n",
        "# Create sampler for post-GRPO inference\n",
        "post_grpo_sampler = sampler_lib.Sampler(\n",
        "    transformer=lora_policy,\n",
        "    tokenizer=tokenizer,\n",
        "    cache_config=sampler_lib.CacheConfig(\n",
        "        cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
        "        num_layers=model_config.num_layers,\n",
        "        num_kv_heads=model_config.num_kv_heads,\n",
        "        head_dim=model_config.head_dim,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Run evaluation on test dataset\n",
        "post_grpo_results = evaluate(test_dataset, post_grpo_sampler, **GENERATION_CONFIGS[\"greedy\"])\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"POST-GRPO EVALUATION RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"  Answer Accuracy:    {post_grpo_results['accuracy']:.2f}%\")\n",
        "print(f\"  Partial Accuracy:   {post_grpo_results['partial_accuracy']:.2f}%\")\n",
        "print(f\"  Format Accuracy:    {post_grpo_results['format_accuracy']:.2f}%\")\n",
        "print(f\"  Correct/Total:      {post_grpo_results['correct']}/{post_grpo_results['total']}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"COMPLETE TRAINING PIPELINE COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"{'Metric':<20} {'Before SFT':<15} {'After SFT':<15} {'After GRPO':<15}\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "print(f\"{'Answer Accuracy':<20} {pre_sft_results['accuracy']:>6.2f}%      {post_sft_results['accuracy']:>6.2f}%      {post_grpo_results['accuracy']:>6.2f}%\")\n",
        "print(f\"{'Partial Accuracy':<20} {pre_sft_results['partial_accuracy']:>6.2f}%      {post_sft_results['partial_accuracy']:>6.2f}%      {post_grpo_results['partial_accuracy']:>6.2f}%\")\n",
        "print(f\"{'Format Accuracy':<20} {pre_sft_results['format_accuracy']:>6.2f}%      {post_sft_results['format_accuracy']:>6.2f}%      {post_grpo_results['format_accuracy']:>6.2f}%\")\n",
        "print(\"=\" * 65)\n",
        "\n",
        "# Calculate total improvements\n",
        "total_acc_improvement = post_grpo_results['accuracy'] - pre_sft_results['accuracy']\n",
        "total_format_improvement = post_grpo_results['format_accuracy'] - pre_sft_results['format_accuracy']\n",
        "\n",
        "print(\"\\nTotal Improvements from Complete Pipeline:\")\n",
        "print(f\"  ✓ Answer Accuracy: {total_acc_improvement:+.2f}% improvement\")\n",
        "print(f\"  ✓ Format Accuracy: {total_format_improvement:+.2f}% improvement\")\n",
        "print(\"=\" * 60 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 8: Export Final Model\n",
        "\n",
        "Export the trained model in a format compatible with Tunix on Kaggle. The exported model:\n",
        "- Merges LoRA weights with the base model\n",
        "- Saves in safetensors format\n",
        "- Includes all necessary config files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 19: Export Merged Model (HuggingFace Format)\n",
        "# ============================================================================\n",
        "\n",
        "output_dir = FINAL_MODEL_DIR\n",
        "if USE_COLAB:\n",
        "    output_dir = f\"/tmp/content/{MODEL_ID.replace('/', '-')}-trained\"\n",
        "\n",
        "# Clean up existing output directory\n",
        "if os.path.exists(output_dir):\n",
        "    shutil.rmtree(output_dir)\n",
        "os.makedirs(output_dir)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"EXPORTING TRAINED MODEL\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nOutput directory: {output_dir}\")\n",
        "\n",
        "# Merge LoRA weights and save\n",
        "print(\"\\nMerging LoRA weights with base model...\")\n",
        "gemma_params.save_lora_merged_model_as_safetensors(\n",
        "    local_model_path=local_model_path,\n",
        "    output_dir=output_dir,\n",
        "    lora_model=lora_policy,\n",
        "    rank=RANK,\n",
        "    alpha=ALPHA,\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"MODEL EXPORTED SUCCESSFULLY!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# List saved files\n",
        "print(\"\\nSaved files:\")\n",
        "for f in sorted(os.listdir(output_dir)):\n",
        "    size = os.path.getsize(os.path.join(output_dir, f)) / (1024 * 1024)\n",
        "    print(f\"  {f:<35} {size:>10.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 20: Download Model (For Colab Users)\n",
        "# ============================================================================\n",
        "\n",
        "if USE_COLAB:\n",
        "    from google.colab import files\n",
        "    \n",
        "    # Create zip archive of the model\n",
        "    zip_path = f\"{output_dir}.zip\"\n",
        "    print(f\"Creating zip archive: {zip_path}\")\n",
        "    shutil.make_archive(output_dir, 'zip', output_dir)\n",
        "    \n",
        "    print(\"Downloading model... (this may take a moment)\")\n",
        "    files.download(zip_path)\n",
        "    print(\"Download complete!\")\n",
        "else:\n",
        "    print(f\"Model saved to: {output_dir}\")\n",
        "    print(\"\\nTo use this model, load it with Tunix:\")\n",
        "    print(f\"  model = params_safetensors_lib.create_model_from_safe_tensors('{output_dir}', config, mesh)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "Congratulations! You have successfully trained a reasoning model using:\n",
        "\n",
        "1. **Cold Start SFT**: Taught the model the output format using `PeftTrainer`\n",
        "   - `<reasoning>model_thinking_trace</reasoning>`\n",
        "   - `<answer>model_answer</answer>`\n",
        "\n",
        "2. **GRPO Reinforcement Learning**: Strengthened reasoning abilities\n",
        "   - Used multiple reward functions\n",
        "   - No separate value model needed (memory efficient!)\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "- **Cold Start is Essential**: Without it, RL training can cause chaotic outputs\n",
        "- **Learning Rate Matters**: SFT uses ~2e-4, GRPO uses ~3e-6 (much lower!)\n",
        "- **Checkpoints are Crucial**: Save frequently to recover from failures\n",
        "- **Format Rewards Help**: Reward proper structure, not just correct answers\n",
        "- **PeftTrainer Approach**: Using Tunix's built-in trainer simplifies SFT training\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. Try different datasets (Bespoke-Stratos-17k, OpenR1-Math-220k)\n",
        "2. Experiment with hyperparameters (NUM_GENERATIONS, BETA, EPSILON)\n",
        "3. Train for more steps for better results\n",
        "4. Evaluate on diverse domains (coding, science, creative writing)\n",
        "\n",
        "### Resources\n",
        "\n",
        "- [Tunix GitHub](https://github.com/google/tunix)\n",
        "- [GRPO Paper](https://arxiv.org/pdf/2402.03300)\n",
        "- [DeepSeek-R1 Technical Report](https://arxiv.org/abs/2401.02954)\n",
        "- [Bespoke-Stratos Dataset](https://huggingface.co/datasets/bespokelabs/Bespoke-Stratos-17k)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
