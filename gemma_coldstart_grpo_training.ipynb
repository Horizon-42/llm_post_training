{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cold Start SFT + GRPO Training for Gemma 3\n",
        "\n",
        "## A Complete Guide to Training Reasoning Models on Kaggle TPU\n",
        "\n",
        "[![Open in Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/google/tunix/blob/main/examples/grpo_gemma.ipynb)\n",
        "\n",
        "---\n",
        "\n",
        "### What This Notebook Does\n",
        "\n",
        "This notebook implements a **two-stage training pipeline** to enhance the reasoning capabilities of the Gemma 3 1B-IT model:\n",
        "\n",
        "1. **Stage 1: Cold Start SFT (Supervised Fine-Tuning)**\n",
        "   - Teaches the model the correct output format: `<reasoning>...</reasoning><answer>...</answer>`\n",
        "   - Plants a \"reasoning template\" into the model using high-quality Chain-of-Thought data\n",
        "   - Prevents the model from producing unreadable or chaotic outputs during RL training\n",
        "\n",
        "2. **Stage 2: GRPO (Group Relative Policy Optimization)**\n",
        "   - Reinforces the model's reasoning abilities through reward-based learning\n",
        "   - Uses multiple reward functions to guide the model toward correct answers and proper formatting\n",
        "   - More memory-efficient than traditional PPO (no need for a separate value model)\n",
        "\n",
        "### Why Two Stages?\n",
        "\n",
        "Research from DeepSeek-R1 shows that pure RL training can cause models to:\n",
        "- Mix languages randomly\n",
        "- Produce unstructured, hard-to-read outputs\n",
        "- Explore inefficiently due to lack of initial guidance\n",
        "\n",
        "The **Cold Start SFT** stage solves these problems by giving the model a \"template\" for how to reason, before we use GRPO to strengthen that reasoning.\n",
        "\n",
        "### Output Format\n",
        "\n",
        "The trained model will produce outputs in this format:\n",
        "```\n",
        "<reasoning>model_thinking_trace</reasoning>\n",
        "<answer>model_answer</answer>\n",
        "```\n",
        "\n",
        "### Hardware Requirements\n",
        "- **Kaggle TPU v6e-1** (recommended) or similar TPU configuration\n",
        "- Training time: ~9 hours for full pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Environment Setup\n",
        "\n",
        "### Understanding the Dependencies\n",
        "\n",
        "Before we start training, we need to install several key libraries:\n",
        "\n",
        "- **tunix**: Google's library for training Gemma models on TPU with JAX\n",
        "- **qwix**: Provides LoRA (Low-Rank Adaptation) functionality for efficient fine-tuning\n",
        "- **flax**: Neural network library for JAX\n",
        "- **grain**: Data loading library optimized for JAX\n",
        "- **transformers**: For tokenizer and model utilities\n",
        "\n",
        "**Important**: After running the installation cell, you may need to restart the kernel for the changes to take effect.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 1: Install Required Packages\n",
        "# ============================================================================\n",
        "# This cell installs all necessary libraries for training.\n",
        "# RESTART THE KERNEL AFTER THIS CELL COMPLETES (for Colab users)\n",
        "\n",
        "import importlib.util\n",
        "\n",
        "def check_package(name):\n",
        "    \"\"\"Check if a package is installed.\"\"\"\n",
        "    return importlib.util.find_spec(name) is not None\n",
        "\n",
        "# Check for key packages - tunix is the most critical one\n",
        "TUNIX_INSTALLED = check_package('tunix')\n",
        "QWIX_INSTALLED = check_package('qwix')\n",
        "\n",
        "if not TUNIX_INSTALLED or not QWIX_INSTALLED:\n",
        "    print(\"Installing required packages... This may take a few minutes.\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Core dependencies\n",
        "    %pip install -q python-dotenv\n",
        "    %pip install -q kagglehub\n",
        "    %pip install -q ipywidgets\n",
        "    %pip install -q tensorflow\n",
        "    %pip install -q tensorflow_datasets\n",
        "    %pip install -q tensorboardX\n",
        "    %pip install -q transformers\n",
        "    %pip install -q grain\n",
        "    \n",
        "    # JAX and related libraries (for TPU training)\n",
        "    %pip install -q git+https://github.com/jax-ml/jax\n",
        "    \n",
        "    # Google's training libraries (CRITICAL - these are the main packages)\n",
        "    %pip install git+https://github.com/google/tunix  # Main training framework\n",
        "    %pip install git+https://github.com/google/qwix   # LoRA support\n",
        "    \n",
        "    # Flax update (required for NNX support)\n",
        "    %pip uninstall -q flax -y\n",
        "    %pip install git+https://github.com/google/flax\n",
        "    \n",
        "    # Data and utilities\n",
        "    %pip install -q huggingface_hub\n",
        "    %pip install -q datasets\n",
        "    %pip install -q 'numpy>2'\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Installation complete!\")\n",
        "    print(\"IMPORTANT: Please RESTART the kernel before continuing.\")\n",
        "    print(\"=\" * 60)\n",
        "else:\n",
        "    print(\"All required packages are already installed.\")\n",
        "    print(f\"  - tunix: {'OK' if TUNIX_INSTALLED else 'MISSING'}\")\n",
        "    print(f\"  - qwix:  {'OK' if QWIX_INSTALLED else 'MISSING'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Authentication Setup\n",
        "\n",
        "This cell handles authentication for:\n",
        "- **Hugging Face**: To download the Gemma model\n",
        "- **Kaggle**: For dataset access\n",
        "- **Weights & Biases (optional)**: For experiment tracking\n",
        "\n",
        "You need to set up your API keys as environment variables or secrets before running this cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 2: Authentication and Service Login\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import kagglehub\n",
        "\n",
        "# Detect if running in Colab or Kaggle environment\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    USE_COLAB = True\n",
        "    \n",
        "    # WandB has issues with Colab, so we disable it\n",
        "    %pip uninstall -q wandb -y\n",
        "    \n",
        "    # Load credentials from Colab secrets\n",
        "    os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
        "    os.environ[\"KAGGLE_USERNAME\"] = userdata.get(\"KAGGLE_USERNAME\")\n",
        "    os.environ[\"KAGGLE_KEY\"] = userdata.get(\"KAGGLE_KEY\")\n",
        "    print(\"Running in Google Colab environment\")\n",
        "    \n",
        "except ImportError:\n",
        "    USE_COLAB = False\n",
        "    \n",
        "    # Try to load from .env file\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "    print(\"Using environment variables for authentication\")\n",
        "    \n",
        "    # Apply nest_asyncio for Jupyter compatibility\n",
        "    import nest_asyncio\n",
        "    nest_asyncio.apply()\n",
        "    print(\"nest_asyncio applied for async compatibility\")\n",
        "    \n",
        "    # Setup WandB for TPU VM (works better outside Colab)\n",
        "    %pip install -q wandb\n",
        "    import wandb\n",
        "    if \"WANDB_API_KEY\" in os.environ and os.environ[\"WANDB_API_KEY\"]:\n",
        "        wandb.login(key=os.environ[\"WANDB_API_KEY\"])\n",
        "        print(\"Weights & Biases login successful\")\n",
        "    else:\n",
        "        print(\"WANDB_API_KEY not found. Skipping W&B login.\")\n",
        "\n",
        "# Kaggle authentication\n",
        "if \"KAGGLE_USERNAME\" not in os.environ or \"KAGGLE_KEY\" not in os.environ:\n",
        "    print(\"Kaggle credentials not found. Please login manually:\")\n",
        "    kagglehub.login()\n",
        "\n",
        "# Hugging Face authentication\n",
        "if \"HF_TOKEN\" in os.environ and os.environ[\"HF_TOKEN\"]:\n",
        "    hf_token = os.environ[\"HF_TOKEN\"]\n",
        "    !huggingface-cli login --token \"$hf_token\"\n",
        "else:\n",
        "    print(\"HF_TOKEN not found. Please set it for model download.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Libraries\n",
        "\n",
        "Now we import all the necessary libraries. Here's what each major import does:\n",
        "\n",
        "- **jax, jnp**: Core library for accelerated numerical computing on TPU/GPU\n",
        "- **flax.nnx**: Neural network library with the new NNX API\n",
        "- **tunix**: Google's library for Gemma model training\n",
        "- **qwix**: LoRA (Low-Rank Adaptation) for efficient fine-tuning\n",
        "- **grain**: Efficient data loading for JAX\n",
        "- **optax**: Gradient transformation and optimization library\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 3: Import All Required Libraries\n",
        "# ============================================================================\n",
        "\n",
        "import functools\n",
        "from pprint import pprint\n",
        "import re\n",
        "import sys\n",
        "import csv\n",
        "import json\n",
        "import shutil\n",
        "\n",
        "# JAX ecosystem\n",
        "from flax import nnx\n",
        "import grain\n",
        "import humanize\n",
        "from huggingface_hub import snapshot_download\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import kagglehub\n",
        "import numpy as np\n",
        "import optax\n",
        "from orbax import checkpoint as ocp\n",
        "from pathlib import Path\n",
        "import qwix\n",
        "import tensorflow_datasets as tfds\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Tunix - Google's Gemma training library\n",
        "from tunix.generate import sampler as sampler_lib\n",
        "from tunix.generate import tokenizer_adapter as tokenizer_lib\n",
        "from tunix.models.gemma3 import model as gemma_lib\n",
        "from tunix.models.gemma3 import params_safetensors as params_safetensors_lib\n",
        "from tunix.models.gemma3 import params as gemma_params\n",
        "from tunix.rl import rl_cluster as rl_cluster_lib\n",
        "from tunix.rl.grpo.grpo_learner import GRPOConfig, GRPOLearner\n",
        "from tunix.rl.rollout import base_rollout\n",
        "from tunix.sft import metrics_logger\n",
        "\n",
        "# Note: tunix doesn't have a built-in SFTTrainer class\n",
        "# We will implement cold start SFT using a custom training loop\n",
        "\n",
        "print(\"All imports successful!\")\n",
        "print(f\"JAX version: {jax.__version__}\")\n",
        "print(f\"Number of devices available: {len(jax.devices())}\")\n",
        "print(f\"Device type: {jax.devices()[0].device_kind}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Hyperparameter Configuration\n",
        "\n",
        "### Understanding the Key Hyperparameters\n",
        "\n",
        "This section defines all the configuration parameters for both SFT and GRPO training. Let's break down the most important ones:\n",
        "\n",
        "**Model Configuration:**\n",
        "- `MODEL_ID`: The base Gemma model to fine-tune\n",
        "- `RANK` and `ALPHA`: LoRA parameters that control the size of trainable adapters\n",
        "\n",
        "**Training Configuration:**\n",
        "- `MAX_SEQ_LENGTH`: Maximum sequence length for training (longer = more memory)\n",
        "- `LEARNING_RATE`: Step size for gradient updates (too high = unstable, too low = slow)\n",
        "\n",
        "**GRPO-Specific Parameters:**\n",
        "- `NUM_GENERATIONS`: How many responses to generate per prompt for comparison (the \"G\" in GRPO)\n",
        "- `BETA`: KL divergence penalty coefficient (keeps the policy close to reference)\n",
        "- `EPSILON`: Clipping parameter for stable updates (similar to PPO)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 4: Hyperparameter Configuration\n",
        "# ============================================================================\n",
        "# All training parameters are defined here for easy modification.\n",
        "# Adjust these based on your hardware and training requirements.\n",
        "\n",
        "# ===========================================================================\n",
        "# MODEL CONFIGURATION\n",
        "# ===========================================================================\n",
        "MODEL_ID = \"google/gemma-3-1b-it\"  # Base model to fine-tune\n",
        "GEMMA_TOKENIZER_PATH = \"gs://gemma-data/tokenizers/tokenizer_gemma3.model\"\n",
        "\n",
        "# ===========================================================================\n",
        "# DATA PATHS\n",
        "# ===========================================================================\n",
        "TRAIN_DATA_DIR = \"./data/train\"\n",
        "TEST_DATA_DIR = \"./data/test\"\n",
        "SFT_DATA_DIR = \"./data/sft\"  # For cold start SFT data\n",
        "TRAIN_FRACTION = 0.9  # Fraction of data used for training (rest for validation)\n",
        "\n",
        "# ===========================================================================\n",
        "# LoRA CONFIGURATION\n",
        "# ===========================================================================\n",
        "# LoRA (Low-Rank Adaptation) allows us to fine-tune large models efficiently\n",
        "# by only training a small number of additional parameters.\n",
        "RANK = 64       # Rank of the LoRA matrices (higher = more parameters, better quality)\n",
        "ALPHA = 64.0    # Scaling factor for LoRA (typically set equal to RANK)\n",
        "\n",
        "# ===========================================================================\n",
        "# SHARDING CONFIGURATION (for TPU)\n",
        "# ===========================================================================\n",
        "# Configure the mesh for distributed training across TPU cores\n",
        "NUM_TPUS = len(jax.devices())\n",
        "print(f\"Detected {NUM_TPUS} TPU cores\")\n",
        "\n",
        "if NUM_TPUS == 8:\n",
        "    MESH_COUNTS = (1, 4)  # For v3-8 TPU\n",
        "elif NUM_TPUS == 4:\n",
        "    MESH_COUNTS = (1, 4)  # For v4-8 TPU\n",
        "elif NUM_TPUS == 1:\n",
        "    MESH_COUNTS = (1, 1)  # Single device\n",
        "else:\n",
        "    # Default configuration for other setups\n",
        "    MESH_COUNTS = (1, NUM_TPUS)\n",
        "    \n",
        "MESH = [MESH_COUNTS, (\"fsdp\", \"tp\")]\n",
        "\n",
        "# ===========================================================================\n",
        "# SEQUENCE LENGTH CONFIGURATION\n",
        "# ===========================================================================\n",
        "MAX_PROMPT_LENGTH = 256           # Maximum length of input prompts\n",
        "TOTAL_GENERATION_STEPS = 768      # Maximum tokens to generate\n",
        "MAX_SEQ_LENGTH = MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS\n",
        "\n",
        "# ===========================================================================\n",
        "# COLD START SFT CONFIGURATION\n",
        "# ===========================================================================\n",
        "SFT_LEARNING_RATE = 2e-4          # Learning rate for SFT (higher than GRPO)\n",
        "SFT_BATCH_SIZE = 2                # Batch size for SFT\n",
        "SFT_GRADIENT_ACCUMULATION = 4     # Simulate larger batch via accumulation\n",
        "SFT_MAX_STEPS = 500               # Number of SFT training steps\n",
        "SFT_WARMUP_STEPS = 50             # Linear warmup steps\n",
        "SFT_SAVE_STEPS = 100              # Save checkpoint every N steps\n",
        "\n",
        "# ===========================================================================\n",
        "# GRPO CONFIGURATION (Based on DeepSeek-R1 Paper)\n",
        "# ===========================================================================\n",
        "# Reference: DeepSeek-R1 uses EPSILON=10, BETA=0.001, NUM_GENERATIONS=16\n",
        "# We adjust values for single TPU memory constraints while following their principles.\n",
        "\n",
        "# Generation parameters during GRPO training\n",
        "TEMPERATURE = 1.0     # DeepSeek-R1: 1.0 for RL Stage 1 (high for exploration)\n",
        "TOP_P = 1.0           # Nucleus sampling parameter\n",
        "TOP_K = 50            # Top-k sampling parameter\n",
        "\n",
        "# GRPO algorithm parameters (adjusted from DeepSeek-R1)\n",
        "NUM_GENERATIONS = 4   # DeepSeek-R1 uses 16, reduced for memory\n",
        "NUM_ITERATIONS = 1    # Number of iterations per batch\n",
        "BETA = 0.001          # KL coefficient (DeepSeek-R1: 0.001, much lower than typical PPO!)\n",
        "EPSILON = 10.0        # Clip ratio (DeepSeek-R1: 10, NOT 0.2 like PPO!)\n",
        "\n",
        "# GRPO training parameters\n",
        "GRPO_LEARNING_RATE = 3e-6         # DeepSeek-R1: 3e-6 âœ“\n",
        "GRPO_BATCH_SIZE = 1               # Keep small due to memory constraints\n",
        "GRPO_GRADIENT_ACCUMULATION = 4    # Effective batch: 4 questions * 4 gen = 16 responses\n",
        "GRPO_MAX_STEPS = 2500             # Number of GRPO training steps\n",
        "GRPO_WARMUP_RATIO = 0.1           # Warmup as fraction of total steps\n",
        "REFERENCE_UPDATE_STEPS = 400      # DeepSeek-R1: Update reference model every 400 steps\n",
        "\n",
        "# ===========================================================================\n",
        "# OPTIMIZER CONFIGURATION\n",
        "# ===========================================================================\n",
        "B1 = 0.9              # Adam beta1\n",
        "B2 = 0.99             # Adam beta2  \n",
        "WEIGHT_DECAY = 0.1    # Weight decay for regularization\n",
        "MAX_GRAD_NORM = 0.1   # Gradient clipping (important for stability)\n",
        "\n",
        "# ===========================================================================\n",
        "# CHECKPOINT CONFIGURATION\n",
        "# ===========================================================================\n",
        "SFT_CKPT_DIR = \"./checkpoints/sft/\"           # SFT checkpoint directory\n",
        "GRPO_CKPT_DIR = \"./checkpoints/grpo/\"         # GRPO checkpoint directory\n",
        "FINAL_MODEL_DIR = \"./checkpoints/final/\"      # Final merged model\n",
        "SAVE_INTERVAL_STEPS = 500                     # Save every N steps\n",
        "MAX_TO_KEEP = 4                               # Maximum checkpoints to keep\n",
        "\n",
        "# ===========================================================================\n",
        "# INFERENCE CONFIGURATION\n",
        "# ===========================================================================\n",
        "GENERATION_CONFIGS = {\n",
        "    \"greedy\": {\"temperature\": None, \"top_k\": 1, \"top_p\": None},      # Deterministic\n",
        "    \"standard\": {\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},    # Balanced\n",
        "    \"creative\": {\"temperature\": 0.85, \"top_k\": 2000, \"top_p\": 1.0},  # More random\n",
        "}\n",
        "\n",
        "print(\"\\nConfiguration loaded successfully!\")\n",
        "print(f\"Model: {MODEL_ID}\")\n",
        "print(f\"LoRA Rank: {RANK}, Alpha: {ALPHA}\")\n",
        "print(f\"SFT Steps: {SFT_MAX_STEPS}, GRPO Steps: {GRPO_MAX_STEPS}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Utility Functions and Special Tokens\n",
        "\n",
        "### Output Format Definition\n",
        "\n",
        "We define a specific output format that the model must learn to use. This format separates:\n",
        "1. **Reasoning**: The model's thought process (between `<reasoning>` and `</reasoning>` tags)\n",
        "2. **Answer**: The final answer (between `<answer>` and `</answer>` tags)\n",
        "\n",
        "This structured format makes it easy to:\n",
        "- Extract and evaluate the model's reasoning\n",
        "- Verify if the final answer is correct\n",
        "- Debug the model's thinking process\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 5: Special Tokens and Template Definition\n",
        "# ============================================================================\n",
        "\n",
        "# Define special tokens for structured output\n",
        "reasoning_start = \"<reasoning>\"\n",
        "reasoning_end = \"</reasoning>\"\n",
        "solution_start = \"<answer>\"\n",
        "solution_end = \"</answer>\"\n",
        "\n",
        "# System prompt that instructs the model on the expected output format\n",
        "SYSTEM_PROMPT = f\"\"\"You are given a problem. First, think about the problem \\\n",
        "and provide your reasoning. Place it between {reasoning_start} and \\\n",
        "{reasoning_end}. Then, provide the final answer between {solution_start} and {solution_end}.\"\"\"\n",
        "\n",
        "# Template for formatting prompts (Gemma chat format)\n",
        "TEMPLATE = \"\"\"<start_of_turn>user\n",
        "{system_prompt}\n",
        "\n",
        "{question}<end_of_turn>\n",
        "<start_of_turn>model\n",
        "\"\"\"\n",
        "\n",
        "# Regex pattern to validate output format\n",
        "match_format = re.compile(\n",
        "    rf\"^[\\s]{{0,}}\"\n",
        "    rf\"{reasoning_start}.+?{reasoning_end}.*?\"\n",
        "    rf\"{solution_start}(.+?){solution_end}\"\n",
        "    rf\"[\\s]{{0,}}$\",\n",
        "    flags=re.MULTILINE | re.DOTALL,\n",
        ")\n",
        "\n",
        "# Regex to extract numbers from answers\n",
        "match_numbers = re.compile(\n",
        "    rf\"{solution_start}.*?([\\d\\.]{{1,}})\", \n",
        "    flags=re.MULTILINE | re.DOTALL\n",
        ")\n",
        "\n",
        "# Test the format matching\n",
        "example = f\"{reasoning_start}Let me think step by step...{reasoning_end}{solution_start}42{solution_end}\"\n",
        "print(\"Example output format:\")\n",
        "print(example)\n",
        "print(f\"\\nFormat validation: {'PASS' if match_format.search(example) else 'FAIL'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 6: Utility Functions\n",
        "# ============================================================================\n",
        "\n",
        "def show_hbm_usage():\n",
        "    \"\"\"Display memory usage per device (useful for debugging OOM issues).\"\"\"\n",
        "    fmt_size = functools.partial(humanize.naturalsize, binary=True)\n",
        "    \n",
        "    for d in jax.local_devices():\n",
        "        stats = d.memory_stats()\n",
        "        used = stats[\"bytes_in_use\"]\n",
        "        limit = stats[\"bytes_limit\"]\n",
        "        print(f\"Device {d}: {fmt_size(used)} / {fmt_size(limit)} ({used/limit:.1%})\")\n",
        "\n",
        "\n",
        "def extract_hash_answer(text: str) -> str:\n",
        "    \"\"\"Extract answer from GSM8K format (after #### marker).\"\"\"\n",
        "    if \"####\" not in text:\n",
        "        return None\n",
        "    return text.split(\"####\")[1].strip()\n",
        "\n",
        "\n",
        "def create_directories():\n",
        "    \"\"\"Create all necessary directories for checkpoints and data.\"\"\"\n",
        "    dirs = [TRAIN_DATA_DIR, TEST_DATA_DIR, SFT_DATA_DIR, \n",
        "            SFT_CKPT_DIR, GRPO_CKPT_DIR, FINAL_MODEL_DIR]\n",
        "    for d in dirs:\n",
        "        os.makedirs(d, exist_ok=True)\n",
        "    print(\"All directories created successfully!\")\n",
        "\n",
        "create_directories()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Model Loading\n",
        "\n",
        "### Loading the Base Gemma Model\n",
        "\n",
        "We download the Gemma 3 1B-IT model from Hugging Face and load it using Tunix. The model is:\n",
        "- **Gemma 3 1B-IT**: A 1 billion parameter instruction-tuned model from Google\n",
        "- Loaded with safetensors format for efficient memory usage\n",
        "- Sharded across TPU cores using the mesh configuration\n",
        "\n",
        "**Note**: You need to have accepted the Gemma license on Kaggle to download the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 7: Download and Load Base Model\n",
        "# ============================================================================\n",
        "\n",
        "# Download model from Hugging Face (skip PyTorch weights)\n",
        "ignore_patterns = [\"*.pth\"]\n",
        "print(f\"Downloading {MODEL_ID} from Hugging Face...\")\n",
        "local_model_path = snapshot_download(\n",
        "    repo_id=MODEL_ID, \n",
        "    ignore_patterns=ignore_patterns\n",
        ")\n",
        "print(f\"Model downloaded to: {local_model_path}\")\n",
        "\n",
        "# Load EOS tokens from generation config\n",
        "EOS_TOKENS = []\n",
        "generation_config_path = os.path.join(local_model_path, \"generation_config.json\")\n",
        "if os.path.exists(generation_config_path):\n",
        "    with open(generation_config_path, \"r\") as f:\n",
        "        generation_configs = json.load(f)\n",
        "    EOS_TOKENS = generation_configs.get(\"eos_token_id\", [])\n",
        "    print(f\"EOS token IDs: {EOS_TOKENS}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 8: Initialize Model with TPU Mesh\n",
        "# ============================================================================\n",
        "\n",
        "# Select model configuration based on MODEL_ID\n",
        "if \"gemma-3-270m\" in MODEL_ID:\n",
        "    model_config = gemma_lib.ModelConfig.gemma3_270m()\n",
        "elif \"gemma-3-1b\" in MODEL_ID:\n",
        "    model_config = gemma_lib.ModelConfig.gemma3_1b_it()\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported model: {MODEL_ID}\")\n",
        "\n",
        "# Create TPU mesh for distributed training\n",
        "mesh = jax.make_mesh(\n",
        "    *MESH, \n",
        "    axis_types=(jax.sharding.AxisType.Auto,) * len(MESH[0])\n",
        ")\n",
        "\n",
        "# Load model with proper sharding\n",
        "print(\"Loading model onto TPU mesh...\")\n",
        "with mesh:\n",
        "    gemma3 = params_safetensors_lib.create_model_from_safe_tensors(\n",
        "        local_model_path, \n",
        "        model_config, \n",
        "        mesh\n",
        "    )\n",
        "\n",
        "print(\"Base model loaded successfully!\")\n",
        "nnx.display(gemma3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LoRA (Low-Rank Adaptation) Setup\n",
        "\n",
        "LoRA is a technique that allows us to fine-tune large models efficiently by:\n",
        "1. Freezing the original model weights\n",
        "2. Adding small trainable \"adapter\" matrices to key layers\n",
        "3. Only training these adapter matrices (much fewer parameters!)\n",
        "\n",
        "**Benefits**:\n",
        "- Reduces memory usage significantly\n",
        "- Faster training\n",
        "- Easy to switch between different fine-tuned versions\n",
        "- Original model weights remain unchanged\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 9: Create LoRA Model and Load Tokenizer\n",
        "# ============================================================================\n",
        "\n",
        "def get_lora_model(base_model, mesh):\n",
        "    \"\"\"\n",
        "    Apply LoRA adapters to the base model.\n",
        "    \n",
        "    LoRA targets specific layers:\n",
        "    - q_einsum, kv_einsum: Attention query/key-value projections\n",
        "    - gate_proj, down_proj, up_proj: MLP layers\n",
        "    - attn_vec_einsum: Attention output projection\n",
        "    \"\"\"\n",
        "    lora_provider = qwix.LoraProvider(\n",
        "        module_path=(\n",
        "            \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|\"\n",
        "            \".*attn_vec_einsum\"\n",
        "        ),\n",
        "        rank=RANK,\n",
        "        alpha=ALPHA,\n",
        "    )\n",
        "    \n",
        "    model_input = base_model.get_model_input()\n",
        "    lora_model = qwix.apply_lora_to_model(\n",
        "        base_model, lora_provider, **model_input\n",
        "    )\n",
        "    \n",
        "    # Shard the LoRA model across devices\n",
        "    with mesh:\n",
        "        state = nnx.state(lora_model)\n",
        "        pspecs = nnx.get_partition_spec(state)\n",
        "        sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
        "        nnx.update(lora_model, sharded_state)\n",
        "    \n",
        "    return lora_model\n",
        "\n",
        "\n",
        "# Create the policy model with LoRA adapters\n",
        "print(\"Creating LoRA policy model...\")\n",
        "lora_policy = get_lora_model(gemma3, mesh=mesh)\n",
        "print(\"LoRA model created!\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = tokenizer_lib.Tokenizer(tokenizer_path=GEMMA_TOKENIZER_PATH)\n",
        "if tokenizer.eos_id() not in EOS_TOKENS:\n",
        "    EOS_TOKENS.append(tokenizer.eos_id())\n",
        "print(f\"Tokenizer loaded. EOS tokens: {EOS_TOKENS}\")\n",
        "\n",
        "# Display memory usage\n",
        "show_hbm_usage()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Data Preparation\n",
        "\n",
        "### Understanding the Two Datasets\n",
        "\n",
        "We use **two different datasets** for the two training stages:\n",
        "\n",
        "---\n",
        "\n",
        "#### Dataset 1: Bespoke-Stratos-17k (for Cold Start SFT)\n",
        "\n",
        "**Source**: `bespokelabs/Bespoke-Stratos-17k` on HuggingFace\n",
        "\n",
        "**Purpose**: Teach the model the reasoning format (`<reasoning>...<answer>`)\n",
        "\n",
        "**Key Features**:\n",
        "- ~17,000 high-quality Chain-of-Thought examples\n",
        "- Distilled from DeepSeek-R1 model\n",
        "- Contains LONG reasoning traces with self-reflection and verification\n",
        "- Includes `<think>...</think>` format (we convert to `<reasoning>...</reasoning>`)\n",
        "- Perfect for cold-start: teaches the model HOW to think, not just WHAT to answer\n",
        "\n",
        "**Why this dataset?**\n",
        "- Shows the model examples of step-by-step reasoning\n",
        "- Teaches proper output structure\n",
        "- Prevents chaotic/unreadable outputs during GRPO\n",
        "\n",
        "---\n",
        "\n",
        "#### Dataset 2: GSM8K / grade-school-math-8k-q-a (for GRPO)\n",
        "\n",
        "**Source**: `thedevastator/grade-school-math-8k-q-a` on Kaggle (or OpenAI's GSM8K)\n",
        "\n",
        "**Purpose**: Strengthen reasoning through reward-based learning\n",
        "\n",
        "**Key Features**:\n",
        "- ~8,000 grade school math word problems\n",
        "- Simple format: question + answer (with `####` separator)\n",
        "- Answers are verifiable (correct or incorrect)\n",
        "- Perfect for GRPO: clear right/wrong signal for rewards\n",
        "\n",
        "**Why this dataset?**\n",
        "- GRPO needs verifiable answers to compute rewards\n",
        "- Math problems have definitive correct answers\n",
        "- Model learns to reason correctly, not just format correctly\n",
        "\n",
        "---\n",
        "\n",
        "### Summary: Two-Stage Data Strategy\n",
        "\n",
        "| Stage | Dataset | Size | Purpose |\n",
        "|-------|---------|------|---------|\n",
        "| **SFT Cold Start** | Bespoke-Stratos-17k | 17k | Learn reasoning FORMAT |\n",
        "| **GRPO Training** | GSM8K | 8k | Strengthen reasoning ACCURACY |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 10: Data Loading Functions\n",
        "# ============================================================================\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# DATASET 1: Bespoke-Stratos-17k for Cold Start SFT\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "def load_bespoke_stratos_dataset(num_examples=None):\n",
        "    \"\"\"\n",
        "    Load Bespoke-Stratos-17k dataset from HuggingFace for Cold Start SFT.\n",
        "    \n",
        "    This dataset contains high-quality Chain-of-Thought examples distilled\n",
        "    from DeepSeek-R1. It's specifically designed for teaching models the\n",
        "    reasoning format.\n",
        "    \n",
        "    Original format uses <think>...</think>, we convert to <reasoning>...</reasoning>\n",
        "    \n",
        "    Args:\n",
        "        num_examples: Maximum number of examples to load (None = all ~17k)\n",
        "    \n",
        "    Returns:\n",
        "        List of dicts with 'prompt' and 'completion' keys\n",
        "    \"\"\"\n",
        "    print(\"Loading Bespoke-Stratos-17k from HuggingFace...\")\n",
        "    \n",
        "    # Load from HuggingFace\n",
        "    dataset = load_dataset(\"bespokelabs/Bespoke-Stratos-17k\", split=\"train\")\n",
        "    \n",
        "    sft_data = []\n",
        "    for i, example in enumerate(tqdm(dataset, desc=\"Processing examples\")):\n",
        "        if num_examples and i >= num_examples:\n",
        "            break\n",
        "        \n",
        "        # Extract conversations\n",
        "        conversations = example.get(\"conversations\", [])\n",
        "        if len(conversations) < 2:\n",
        "            continue\n",
        "        \n",
        "        # Get user message and assistant response\n",
        "        user_msg = None\n",
        "        assistant_msg = None\n",
        "        \n",
        "        for conv in conversations:\n",
        "            role = conv.get(\"from\", conv.get(\"role\", \"\"))\n",
        "            content = conv.get(\"value\", conv.get(\"content\", \"\"))\n",
        "            \n",
        "            if role in [\"human\", \"user\"]:\n",
        "                user_msg = content\n",
        "            elif role in [\"gpt\", \"assistant\"]:\n",
        "                assistant_msg = content\n",
        "        \n",
        "        if not user_msg or not assistant_msg:\n",
        "            continue\n",
        "        \n",
        "        # Convert various thinking tags to our standard <reasoning>...</reasoning> format\n",
        "        # Bespoke-Stratos uses: <|begin_of_thought|>...<|end_of_thought|>\n",
        "        # Some datasets use: <think>...</think>\n",
        "        completion = assistant_msg\n",
        "        \n",
        "        # Handle Bespoke-Stratos format\n",
        "        completion = completion.replace(\"<|begin_of_thought|>\", reasoning_start)\n",
        "        completion = completion.replace(\"<|end_of_thought|>\", reasoning_end)\n",
        "        completion = completion.replace(\"<|begin_of_solution|>\", solution_start)\n",
        "        completion = completion.replace(\"<|end_of_solution|>\", solution_end)\n",
        "        \n",
        "        # Handle other common formats\n",
        "        completion = completion.replace(\"<think>\", reasoning_start)\n",
        "        completion = completion.replace(\"</think>\", reasoning_end)\n",
        "        \n",
        "        # If still no answer tags, add them\n",
        "        if solution_start not in completion:\n",
        "            # Try to extract final answer after reasoning\n",
        "            if reasoning_end in completion:\n",
        "                parts = completion.split(reasoning_end)\n",
        "                if len(parts) > 1 and parts[1].strip():\n",
        "                    # Wrap the part after reasoning in answer tags\n",
        "                    completion = parts[0] + reasoning_end + \"\\n\" + solution_start + parts[1].strip() + solution_end\n",
        "                else:\n",
        "                    completion = completion + f\"\\n{solution_start}See reasoning above{solution_end}\"\n",
        "            else:\n",
        "                completion = completion + f\"\\n{solution_start}See reasoning above{solution_end}\"\n",
        "        \n",
        "        # Format prompt with our template\n",
        "        prompt = TEMPLATE.format(\n",
        "            system_prompt=SYSTEM_PROMPT,\n",
        "            question=user_msg,\n",
        "        )\n",
        "        \n",
        "        sft_data.append({\n",
        "            \"prompt\": prompt,\n",
        "            \"completion\": completion,\n",
        "        })\n",
        "    \n",
        "    print(f\"Loaded {len(sft_data)} SFT examples from Bespoke-Stratos-17k\")\n",
        "    return sft_data\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# DATASET 2: GSM8K for GRPO Training\n",
        "# ----------------------------------------------------------------------------\n",
        "\n",
        "def download_kaggle_dataset(target_dir=\"./data/gsm8k\"):\n",
        "    \"\"\"Download GSM8K dataset from Kaggle.\"\"\"\n",
        "    os.makedirs(target_dir, exist_ok=True)\n",
        "    src = kagglehub.dataset_download(\"thedevastator/grade-school-math-8k-q-a\")\n",
        "    src = Path(src)\n",
        "    dst = Path(target_dir)\n",
        "    \n",
        "    for csv_file in src.glob(\"*.csv\"):\n",
        "        shutil.copy2(csv_file, dst / csv_file.name)\n",
        "        print(f\"Copied {csv_file.name}\")\n",
        "    return target_dir\n",
        "\n",
        "\n",
        "def get_grpo_dataset(data_dir, split=\"train\", source=\"kaggle\") -> grain.MapDataset:\n",
        "    \"\"\"\n",
        "    Load and format dataset for GRPO training.\n",
        "    \n",
        "    Returns a dataset with:\n",
        "    - prompts: Formatted input prompts\n",
        "    - question: Original question text  \n",
        "    - answer: Ground truth answer for verification\n",
        "    \"\"\"\n",
        "    if source == \"tfds\":\n",
        "        import tensorflow_datasets.text.gsm8k\n",
        "        data = tfds.data_source(\n",
        "            \"gsm8k\",\n",
        "            split=split,\n",
        "            data_dir=data_dir,\n",
        "            builder_kwargs={\"file_format\": tfds.core.FileFormat.ARRAY_RECORD},\n",
        "            download=True,\n",
        "        )\n",
        "    elif source == \"kaggle\":\n",
        "        kaggle_dir = download_kaggle_dataset(data_dir)\n",
        "        file_name = \"main_\" + split + \".csv\"\n",
        "        csv_path = os.path.join(kaggle_dir, file_name)\n",
        "        \n",
        "        data = []\n",
        "        with open(csv_path, newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "            reader = csv.DictReader(csvfile)\n",
        "            for row in reader:\n",
        "                data.append({\n",
        "                    \"question\": row[\"question\"],\n",
        "                    \"answer\": row[\"answer\"],\n",
        "                })\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown source: {source}\")\n",
        "    \n",
        "    def _as_text(v):\n",
        "        return v if isinstance(v, str) else v.decode(\"utf-8\")\n",
        "    \n",
        "    dataset = (\n",
        "        grain.MapDataset.source(data)\n",
        "        .shuffle(seed=42)\n",
        "        .map(lambda x: {\n",
        "            \"prompts\": TEMPLATE.format(\n",
        "                system_prompt=SYSTEM_PROMPT,\n",
        "                question=_as_text(x[\"question\"]),\n",
        "            ),\n",
        "            \"question\": _as_text(x[\"question\"]),\n",
        "            \"answer\": extract_hash_answer(_as_text(x[\"answer\"])),\n",
        "        })\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def create_sft_dataset(num_examples=1000):\n",
        "    \"\"\"\n",
        "    Create SFT dataset for cold start training.\n",
        "    \n",
        "    This generates training examples that teach the model:\n",
        "    1. The correct output format (<reasoning>...</reasoning><answer>...</answer>)\n",
        "    2. Step-by-step reasoning patterns\n",
        "    \n",
        "    In practice, you would use a high-quality dataset like Bespoke-Stratos-17k\n",
        "    or OpenR1-Mixture-of-Thoughts.\n",
        "    \"\"\"\n",
        "    # For demonstration, we'll convert GSM8K examples into SFT format\n",
        "    # In production, use a dedicated reasoning dataset\n",
        "    \n",
        "    print(\"Creating SFT dataset from GSM8K with formatted examples...\")\n",
        "    \n",
        "    # Load base data\n",
        "    kaggle_dir = download_kaggle_dataset(SFT_DATA_DIR)\n",
        "    csv_path = os.path.join(kaggle_dir, \"main_train.csv\")\n",
        "    \n",
        "    sft_data = []\n",
        "    with open(csv_path, newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        for i, row in enumerate(reader):\n",
        "            if i >= num_examples:\n",
        "                break\n",
        "                \n",
        "            question = row[\"question\"]\n",
        "            full_answer = row[\"answer\"]\n",
        "            \n",
        "            # Extract the reasoning (everything before ####) and answer\n",
        "            if \"####\" in full_answer:\n",
        "                reasoning_part = full_answer.split(\"####\")[0].strip()\n",
        "                answer_part = full_answer.split(\"####\")[1].strip()\n",
        "            else:\n",
        "                reasoning_part = full_answer\n",
        "                answer_part = \"N/A\"\n",
        "            \n",
        "            # Format as training example\n",
        "            formatted_output = f\"{reasoning_start}\\n{reasoning_part}\\n{reasoning_end}\\n{solution_start}{answer_part}{solution_end}\"\n",
        "            \n",
        "            sft_data.append({\n",
        "                \"prompt\": TEMPLATE.format(\n",
        "                    system_prompt=SYSTEM_PROMPT,\n",
        "                    question=question,\n",
        "                ),\n",
        "                \"completion\": formatted_output,\n",
        "            })\n",
        "    \n",
        "    print(f\"Created {len(sft_data)} SFT examples\")\n",
        "    return sft_data\n",
        "\n",
        "\n",
        "print(\"Data loading functions defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Stage 1 - Cold Start SFT Training\n",
        "\n",
        "Cold Start SFT teaches the model the output format before GRPO strengthens reasoning.\n",
        "\n",
        "Key goals:\n",
        "- Teach the `<reasoning>` and `<answer>` tag format\n",
        "- Show examples of step-by-step thinking\n",
        "- Prevent chaotic outputs during later RL training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 10.4: Reload Model from Original Checkpoint (for Loop Testing)\n",
        "# ============================================================================\n",
        "# This cell reloads the model from the original downloaded model (not from checkpoints).\n",
        "# Use this cell to reset the model state before running training loops.\n",
        "# \n",
        "# This is useful for:\n",
        "# - Testing different hyperparameters\n",
        "# - Comparing different training runs\n",
        "# - Resetting model to baseline before each experiment\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"RELOADING MODEL FROM ORIGINAL CHECKPOINT\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Reloading from: {local_model_path}\")\n",
        "print(\"This will reset all LoRA parameters to initial state.\\n\")\n",
        "\n",
        "# Ensure mesh is available (it should be from CELL 8)\n",
        "if 'mesh' not in locals():\n",
        "    print(\"Creating TPU mesh...\")\n",
        "    mesh = jax.make_mesh(\n",
        "        *MESH, \n",
        "        axis_types=(jax.sharding.AxisType.Auto,) * len(MESH[0])\n",
        "    )\n",
        "\n",
        "# Select model configuration based on MODEL_ID\n",
        "if \"gemma-3-270m\" in MODEL_ID:\n",
        "    model_config = gemma_lib.ModelConfig.gemma3_270m()\n",
        "elif \"gemma-3-1b\" in MODEL_ID:\n",
        "    model_config = gemma_lib.ModelConfig.gemma3_1b_it()\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported model: {MODEL_ID}\")\n",
        "\n",
        "# Reload base model from original checkpoint\n",
        "print(\"Loading base model from original checkpoint...\")\n",
        "with mesh:\n",
        "    gemma3 = params_safetensors_lib.create_model_from_safe_tensors(\n",
        "        local_model_path, \n",
        "        model_config, \n",
        "        mesh\n",
        "    )\n",
        "\n",
        "print(\"Base model reloaded successfully!\")\n",
        "\n",
        "# Recreate LoRA model (fresh LoRA adapters, all weights reset)\n",
        "print(\"Recreating LoRA policy model...\")\n",
        "lora_policy = get_lora_model(gemma3, mesh=mesh)\n",
        "print(\"LoRA model recreated with fresh adapters!\")\n",
        "\n",
        "# Reload tokenizer (if needed)\n",
        "if 'tokenizer' not in locals():\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tokenizer = tokenizer_lib.Tokenizer(tokenizer_path=GEMMA_TOKENIZER_PATH)\n",
        "    if tokenizer.eos_id() not in EOS_TOKENS:\n",
        "        EOS_TOKENS.append(tokenizer.eos_id())\n",
        "    print(f\"Tokenizer loaded. EOS tokens: {EOS_TOKENS}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"MODEL RESET COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Model has been reset to original state.\")\n",
        "print(\"All previous training has been cleared.\")\n",
        "print(\"Ready for fresh training run.\")\n",
        "print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "# Display memory usage\n",
        "show_hbm_usage()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 10.5: Load Evaluation Dataset and Define Evaluation Functions\n",
        "# ============================================================================\n",
        "# Load a small test dataset for evaluation before and after SFT training.\n",
        "# Also define evaluate and generate functions here for use in comparison cells.\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"PREPARING EVALUATION DATASET\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load a small test dataset for evaluation (before and after SFT)\n",
        "NUM_EVAL_SAMPLES = 32  # Small sample for quick evaluation\n",
        "eval_test_dataset = get_grpo_dataset(TEST_DATA_DIR, \"test\", \"kaggle\")\n",
        "eval_test_dataset = eval_test_dataset.batch(1)[:NUM_EVAL_SAMPLES]\n",
        "\n",
        "print(f\"Loaded {NUM_EVAL_SAMPLES} test samples for evaluation\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Define evaluation functions (same as in CELL 20, but defined here for early use)\n",
        "def generate(question, sampler, temperature=0.7, top_k=50, top_p=0.95, seed=None):\n",
        "    \"\"\"Generate response for a given question.\"\"\"\n",
        "    if isinstance(question, str):\n",
        "        input_batch = [TEMPLATE.format(system_prompt=SYSTEM_PROMPT, question=question)]\n",
        "    else:\n",
        "        input_batch = [\n",
        "            TEMPLATE.format(system_prompt=SYSTEM_PROMPT, question=q)\n",
        "            for q in question\n",
        "        ]\n",
        "    \n",
        "    out_data = sampler(\n",
        "        input_strings=input_batch,\n",
        "        max_generation_steps=TOTAL_GENERATION_STEPS,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        echo=False,\n",
        "        seed=seed,\n",
        "        eos_tokens=EOS_TOKENS,\n",
        "    )\n",
        "    \n",
        "    output = out_data.text\n",
        "    return output[0] if isinstance(question, str) else output\n",
        "\n",
        "\n",
        "def evaluate(dataset, sampler, temperature=0.7, top_k=50, top_p=0.95):\n",
        "    \"\"\"Evaluate model on dataset and compute metrics.\"\"\"\n",
        "    correct = 0\n",
        "    partially_correct = 0\n",
        "    correct_format = 0\n",
        "    total = 0\n",
        "    \n",
        "    for batch in tqdm(dataset, desc=\"Evaluating\"):\n",
        "        answers = batch[\"answer\"]\n",
        "        questions = batch[\"question\"]\n",
        "        \n",
        "        responses = generate(questions, sampler, temperature, top_k, top_p)\n",
        "        \n",
        "        for question, response, answer in zip(questions, responses, answers):\n",
        "            # Check answer\n",
        "            extracted = match_numbers.search(response)\n",
        "            if extracted:\n",
        "                try:\n",
        "                    pred = float(extracted.group(1).strip())\n",
        "                    true_val = float(answer.strip())\n",
        "                    if pred == true_val:\n",
        "                        correct += 1\n",
        "                    ratio = pred / true_val if true_val != 0 else 0\n",
        "                    if 0.9 <= ratio <= 1.1:\n",
        "                        partially_correct += 1\n",
        "                except:\n",
        "                    pass\n",
        "            \n",
        "            # Check format\n",
        "            if match_format.search(response):\n",
        "                correct_format += 1\n",
        "            \n",
        "            total += 1\n",
        "            \n",
        "            # Print progress\n",
        "            if total % 10 == 0:\n",
        "                print(f\"Progress: {correct}/{total} correct ({correct/total*100:.1f}%)\")\n",
        "    \n",
        "    return {\n",
        "        \"accuracy\": correct / total * 100 if total > 0 else 0,\n",
        "        \"partial_accuracy\": partially_correct / total * 100 if total > 0 else 0,\n",
        "        \"format_accuracy\": correct_format / total * 100 if total > 0 else 0,\n",
        "        \"correct\": correct,\n",
        "        \"total\": total,\n",
        "    }\n",
        "\n",
        "print(\"\\nEvaluation functions defined!\")\n",
        "print(\"Ready for pre-SFT and post-SFT evaluation.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 11: Prepare SFT Dataset (Bespoke-Stratos-17k)\n",
        "# ============================================================================\n",
        "# We use Bespoke-Stratos-17k for cold start SFT - this dataset contains\n",
        "# high-quality Chain-of-Thought examples that teach proper reasoning format.\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Loading SFT Dataset: Bespoke-Stratos-17k\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load Bespoke-Stratos-17k from HuggingFace\n",
        "# Set num_examples to limit for faster training (None = use all ~17k)\n",
        "SFT_NUM_EXAMPLES = 2000  # Adjust based on available time\n",
        "\n",
        "sft_data = load_bespoke_stratos_dataset(num_examples=SFT_NUM_EXAMPLES)\n",
        "\n",
        "# Convert to grain dataset format\n",
        "def format_sft_example(example):\n",
        "    \"\"\"Combine prompt and completion into a single training text.\"\"\"\n",
        "    return {\"text\": example[\"prompt\"] + example[\"completion\"]}\n",
        "\n",
        "sft_dataset = (\n",
        "    grain.MapDataset.source(sft_data)\n",
        "    .shuffle(seed=42)\n",
        "    .map(format_sft_example)\n",
        "    .batch(SFT_BATCH_SIZE)\n",
        ")\n",
        "\n",
        "# Preview an example\n",
        "print(\"Sample SFT training example:\")\n",
        "print(\"=\" * 60)\n",
        "sample = sft_data[0]\n",
        "print(f\"PROMPT:\\n{sample['prompt'][:200]}...\")\n",
        "print(f\"\\nCOMPLETION:\\n{sample['completion'][:300]}...\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 11.5: Evaluate Model Before SFT Training\n",
        "# ============================================================================\n",
        "# This cell evaluates the model's performance BEFORE cold start SFT training.\n",
        "# Uses the evaluate function to get quantitative metrics.\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"EVALUATING MODEL BEFORE SFT TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nRunning evaluation on test dataset...\")\n",
        "print(\"This will measure accuracy and format compliance before training.\\n\")\n",
        "\n",
        "# Create a sampler for inference\n",
        "pre_sft_sampler = sampler_lib.Sampler(\n",
        "    transformer=lora_policy,\n",
        "    tokenizer=tokenizer,\n",
        "    cache_config=sampler_lib.CacheConfig(\n",
        "        cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
        "        num_layers=model_config.num_layers,\n",
        "        num_kv_heads=model_config.num_kv_heads,\n",
        "        head_dim=model_config.head_dim,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Run evaluation\n",
        "pre_sft_results = evaluate(eval_test_dataset, pre_sft_sampler, **GENERATION_CONFIGS[\"greedy\"])\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PRE-SFT EVALUATION RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"  Answer Accuracy:    {pre_sft_results['accuracy']:.2f}%\")\n",
        "print(f\"  Partial Accuracy:   {pre_sft_results['partial_accuracy']:.2f}%\")\n",
        "print(f\"  Format Accuracy:    {pre_sft_results['format_accuracy']:.2f}%\")\n",
        "print(f\"  Correct/Total:      {pre_sft_results['correct']}/{pre_sft_results['total']}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"READY FOR SFT TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "print(\"After SFT training, we expect improvements in:\")\n",
        "print(\"  - Format Accuracy: Model should learn <reasoning>...</reasoning><answer>...</answer> format\")\n",
        "print(\"  - Answer Accuracy: Better reasoning may lead to more correct answers\")\n",
        "print(\"=\" * 60 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 12: Run Cold Start SFT Training\n",
        "# ============================================================================\n",
        "# This stage teaches the model the reasoning format using Bespoke-Stratos-17k.\n",
        "# Cold Start is ESSENTIAL - it prevents chaotic outputs during GRPO training.\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"STAGE 1: COLD START SFT TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nUsing Bespoke-Stratos-17k dataset to teach reasoning format.\")\n",
        "print(\"This stage plants the <reasoning>...</reasoning><answer>...</answer> template.\")\n",
        "\n",
        "# Ensure we have enough steps for warmup + decay\n",
        "actual_sft_warmup = min(SFT_WARMUP_STEPS, SFT_MAX_STEPS // 10)\n",
        "actual_sft_decay = max(SFT_MAX_STEPS, actual_sft_warmup + 100)\n",
        "\n",
        "print(f\"Adjusted schedule: warmup={actual_sft_warmup}, total_decay={actual_sft_decay}\")\n",
        "\n",
        "# Configure SFT optimizer with warmup and cosine decay\n",
        "sft_optax_optimizer = optax.chain(\n",
        "    optax.clip_by_global_norm(max_norm=MAX_GRAD_NORM),\n",
        "    optax.adamw(\n",
        "        learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n",
        "            init_value=0.0,\n",
        "            peak_value=SFT_LEARNING_RATE,\n",
        "            warmup_steps=actual_sft_warmup,\n",
        "            decay_steps=actual_sft_decay,\n",
        "            end_value=0.0,\n",
        "        ),\n",
        "        b1=B1,\n",
        "        b2=B2,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Use NNX Optimizer which properly handles state management\n",
        "# wrt=nnx.LoRAParam specifies we only optimize LoRA parameters (not full model)\n",
        "sft_optimizer = nnx.Optimizer(lora_policy, sft_optax_optimizer, wrt=nnx.LoRAParam)\n",
        "\n",
        "# Convert checkpoint directory to absolute path (required by Orbax)\n",
        "SFT_CKPT_DIR_ABS = os.path.abspath(SFT_CKPT_DIR)\n",
        "os.makedirs(SFT_CKPT_DIR_ABS, exist_ok=True)\n",
        "\n",
        "# Checkpoint manager for SFT\n",
        "sft_ckpt_manager = ocp.CheckpointManager(\n",
        "    SFT_CKPT_DIR_ABS,\n",
        "    options=ocp.CheckpointManagerOptions(\n",
        "        save_interval_steps=SFT_SAVE_STEPS,\n",
        "        max_to_keep=MAX_TO_KEEP,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(f\"\\nSFT Configuration:\")\n",
        "print(f\"  Learning rate: {SFT_LEARNING_RATE}\")\n",
        "print(f\"  Max steps: {SFT_MAX_STEPS}\")\n",
        "print(f\"  Batch size: {SFT_BATCH_SIZE}\")\n",
        "print(f\"  Checkpoint dir: {SFT_CKPT_DIR_ABS}\")\n",
        "\n",
        "\n",
        "# Define the training step function using NNX patterns\n",
        "@nnx.jit\n",
        "def sft_train_step(model, optimizer, tokens, loss_mask):\n",
        "    \"\"\"\n",
        "    Single SFT training step using NNX.\n",
        "    \n",
        "    This function is JIT-compiled for efficiency.\n",
        "    \"\"\"\n",
        "    def loss_fn(model):\n",
        "        # Get sequence length for position calculation\n",
        "        batch_size = tokens.shape[0]\n",
        "        seq_len = tokens.shape[1] - 1\n",
        "        \n",
        "        # Create position indices\n",
        "        positions = jnp.arange(seq_len)[None, :]  # Shape: (1, seq_len)\n",
        "        positions = jnp.broadcast_to(positions, (batch_size, seq_len))\n",
        "        \n",
        "        # Create causal attention mask (lower triangular)\n",
        "        # Shape: (seq_len, seq_len) - True where attention is allowed\n",
        "        causal_mask = jnp.tril(jnp.ones((seq_len, seq_len), dtype=jnp.bool_))\n",
        "        \n",
        "        # Forward pass - get logits for all positions except last\n",
        "        # Model returns (cache, logits) tuple\n",
        "        output = model(\n",
        "            tokens[:, :-1],\n",
        "            positions,\n",
        "            cache=None,\n",
        "            attention_mask=causal_mask\n",
        "        )\n",
        "        # Extract logits from output\n",
        "        # Gemma3 model returns different formats - find the logits array\n",
        "        if isinstance(output, tuple):\n",
        "            # Try to find the logits (should be the array with shape [batch, seq, vocab])\n",
        "            for item in output:\n",
        "                if item is not None and hasattr(item, 'shape') and len(item.shape) == 3:\n",
        "                    logits = item\n",
        "                    break\n",
        "            else:\n",
        "                # If not found, try first non-None element\n",
        "                logits = output[0] if output[0] is not None else output[1]\n",
        "        else:\n",
        "            logits = output\n",
        "        \n",
        "        # Targets are the next tokens\n",
        "        targets = tokens[:, 1:]\n",
        "        \n",
        "        # Cross-entropy loss\n",
        "        log_probs = jax.nn.log_softmax(logits, axis=-1)\n",
        "        target_log_probs = jnp.take_along_axis(\n",
        "            log_probs, targets[:, :, None], axis=-1\n",
        "        ).squeeze(-1)\n",
        "        \n",
        "        # Apply loss mask (to ignore padding tokens)\n",
        "        loss = -jnp.sum(target_log_probs * loss_mask) / (jnp.sum(loss_mask) + 1e-8)\n",
        "        return loss\n",
        "    \n",
        "    # Compute loss and gradients, then update\n",
        "    # Flax 0.11.0+: optimizer.update requires (model, grads)\n",
        "    loss, grads = nnx.value_and_grad(loss_fn)(model)\n",
        "    optimizer.update(model, grads)\n",
        "    \n",
        "    return loss\n",
        "\n",
        "\n",
        "# Initialize loss tracking for visualization\n",
        "sft_loss_history = []  # List to store (step, loss) pairs\n",
        "sft_log_dir = \"./logs/sft\"\n",
        "\n",
        "# Create log directory for TensorBoard\n",
        "os.makedirs(sft_log_dir, exist_ok=True)\n",
        "\n",
        "# Initialize TensorBoard SummaryWriter (if tensorboardX is available)\n",
        "try:\n",
        "    from tensorboardX import SummaryWriter\n",
        "    tb_writer = SummaryWriter(log_dir=sft_log_dir)\n",
        "    use_tensorboard = True\n",
        "    print(f\"TensorBoard logging enabled. Logs saved to: {sft_log_dir}\")\n",
        "except ImportError:\n",
        "    use_tensorboard = False\n",
        "    print(\"tensorboardX not available. Loss will only be stored in memory for plotting.\")\n",
        "\n",
        "# Training loop\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Starting SFT training for {SFT_MAX_STEPS} steps...\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "step = 0\n",
        "total_loss = 0.0\n",
        "num_epochs = max(1, (SFT_MAX_STEPS * SFT_BATCH_SIZE) // len(sft_data) + 1)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\n--- Epoch {epoch + 1}/{num_epochs} ---\")\n",
        "    \n",
        "    for batch in tqdm(sft_dataset, desc=f\"Epoch {epoch+1}\"):\n",
        "        if step >= SFT_MAX_STEPS:\n",
        "            break\n",
        "        \n",
        "        # Get texts from batch\n",
        "        texts = batch[\"text\"]\n",
        "        if isinstance(texts, np.ndarray):\n",
        "            texts = texts.tolist()\n",
        "        \n",
        "        # Tokenize batch\n",
        "        tokens_list = [tokenizer.encode(t) for t in texts]\n",
        "        \n",
        "        # Pad sequences\n",
        "        max_len = min(MAX_SEQ_LENGTH, max(len(t) for t in tokens_list))\n",
        "        padded_tokens = np.zeros((len(tokens_list), max_len), dtype=np.int32)\n",
        "        attention_mask = np.zeros((len(tokens_list), max_len), dtype=np.float32)\n",
        "        \n",
        "        for i, toks in enumerate(tokens_list):\n",
        "            length = min(len(toks), max_len)\n",
        "            padded_tokens[i, :length] = toks[:length]\n",
        "            attention_mask[i, :length] = 1.0\n",
        "        \n",
        "        # Convert to JAX arrays\n",
        "        tokens_jax = jnp.array(padded_tokens)\n",
        "        # Loss mask is for the TARGET tokens (shifted by 1 from input)\n",
        "        loss_mask_jax = jnp.array(attention_mask[:, 1:])\n",
        "        \n",
        "        # Training step\n",
        "        loss = sft_train_step(lora_policy, sft_optimizer, tokens_jax, loss_mask_jax)\n",
        "        \n",
        "        loss_value = float(loss)\n",
        "        total_loss += loss_value\n",
        "        step += 1\n",
        "        \n",
        "        # Record loss for visualization\n",
        "        sft_loss_history.append((step, loss_value))\n",
        "        \n",
        "        # Log to TensorBoard\n",
        "        if use_tensorboard:\n",
        "            tb_writer.add_scalar('train/loss', loss_value, step)\n",
        "            tb_writer.add_scalar('train/avg_loss', total_loss / step, step)\n",
        "        \n",
        "        # Console logging\n",
        "        if step % 20 == 0:\n",
        "            avg_loss = total_loss / step\n",
        "            print(f\"Step {step}/{SFT_MAX_STEPS} | Loss: {loss_value:.4f} | Avg Loss: {avg_loss:.4f}\")\n",
        "        \n",
        "        # Save checkpoint\n",
        "        if step % SFT_SAVE_STEPS == 0:\n",
        "            print(f\"\\n[Checkpoint] Saving at step {step}...\")\n",
        "            sft_ckpt_manager.save(\n",
        "                step,\n",
        "                args=ocp.args.StandardSave(nnx.state(lora_policy, nnx.LoRAParam)),\n",
        "            )\n",
        "            print(f\"[Checkpoint] Saved to {SFT_CKPT_DIR_ABS}\")\n",
        "        \n",
        "        if step >= SFT_MAX_STEPS:\n",
        "            break\n",
        "    \n",
        "    if step >= SFT_MAX_STEPS:\n",
        "        break\n",
        "\n",
        "# Save final SFT checkpoint\n",
        "print(f\"\\n[Final Checkpoint] Saving final SFT model...\")\n",
        "sft_ckpt_manager.save(\n",
        "    step,\n",
        "    args=ocp.args.StandardSave(nnx.state(lora_policy, nnx.LoRAParam)),\n",
        ")\n",
        "\n",
        "# Close TensorBoard writer\n",
        "if use_tensorboard:\n",
        "    tb_writer.close()\n",
        "    print(f\"\\nTensorBoard logs saved to: {sft_log_dir}\")\n",
        "    print(\"To view: tensorboard --logdir ./logs/sft\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"COLD START SFT TRAINING COMPLETE!\")\n",
        "print(f\"Final Loss: {total_loss / step:.4f}\")\n",
        "print(f\"Checkpoints saved to: {SFT_CKPT_DIR_ABS}\")\n",
        "print(f\"Loss history recorded: {len(sft_loss_history)} steps\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "show_hbm_usage()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 12.5: Visualize Training Results & Compare Model Output After SFT\n",
        "# ============================================================================\n",
        "# This cell:\n",
        "# 1. Plots the loss curve from SFT training\n",
        "# 2. Shows model output AFTER SFT training for comparison\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"SFT TRAINING RESULTS VISUALIZATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 1. Plot Loss Curve\n",
        "if len(sft_loss_history) > 0:\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    steps, losses = zip(*sft_loss_history)\n",
        "    \n",
        "    # Create figure with subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Plot 1: Raw loss over steps\n",
        "    ax1.plot(steps, losses, 'b-', alpha=0.6, linewidth=1, label='Step Loss')\n",
        "    \n",
        "    # Add moving average for smoother visualization\n",
        "    window_size = max(10, len(losses) // 20)\n",
        "    if len(losses) >= window_size:\n",
        "        moving_avg = []\n",
        "        for i in range(len(losses)):\n",
        "            start_idx = max(0, i - window_size // 2)\n",
        "            end_idx = min(len(losses), i + window_size // 2 + 1)\n",
        "            moving_avg.append(np.mean(losses[start_idx:end_idx]))\n",
        "        ax1.plot(steps, moving_avg, 'r-', linewidth=2, label=f'Moving Avg (window={window_size})')\n",
        "    \n",
        "    ax1.set_xlabel('Training Step')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.set_title('SFT Training Loss Over Time')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 2: Average loss (cumulative average)\n",
        "    avg_losses = [np.mean(losses[:i+1]) for i in range(len(losses))]\n",
        "    ax2.plot(steps, avg_losses, 'g-', linewidth=2, label='Cumulative Average Loss')\n",
        "    ax2.set_xlabel('Training Step')\n",
        "    ax2.set_ylabel('Average Loss')\n",
        "    ax2.set_title('Cumulative Average Loss')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\nLoss Statistics:\")\n",
        "    print(f\"  Initial Loss: {losses[0]:.4f}\")\n",
        "    print(f\"  Final Loss: {losses[-1]:.4f}\")\n",
        "    print(f\"  Average Loss: {np.mean(losses):.4f}\")\n",
        "    print(f\"  Min Loss: {np.min(losses):.4f} (at step {steps[np.argmin(losses)]})\")\n",
        "    print(f\"  Loss Reduction: {((losses[0] - losses[-1]) / losses[0] * 100):.2f}%\")\n",
        "else:\n",
        "    print(\"Warning: No loss history recorded. Skipping loss plot.\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"EVALUATING MODEL AFTER SFT TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nRunning evaluation on the same test dataset...\")\n",
        "print(\"This will show quantitative improvements after training.\\n\")\n",
        "\n",
        "# Create sampler for post-SFT inference\n",
        "post_sft_sampler = sampler_lib.Sampler(\n",
        "    transformer=lora_policy,\n",
        "    tokenizer=tokenizer,\n",
        "    cache_config=sampler_lib.CacheConfig(\n",
        "        cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
        "        num_layers=model_config.num_layers,\n",
        "        num_kv_heads=model_config.num_kv_heads,\n",
        "        head_dim=model_config.head_dim,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Run evaluation on the same dataset\n",
        "post_sft_results = evaluate(eval_test_dataset, post_sft_sampler, **GENERATION_CONFIGS[\"greedy\"])\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"POST-SFT EVALUATION RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"  Answer Accuracy:    {post_sft_results['accuracy']:.2f}%\")\n",
        "print(f\"  Partial Accuracy:   {post_sft_results['partial_accuracy']:.2f}%\")\n",
        "print(f\"  Format Accuracy:    {post_sft_results['format_accuracy']:.2f}%\")\n",
        "print(f\"  Correct/Total:      {post_sft_results['correct']}/{post_sft_results['total']}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"BEFORE vs AFTER COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"{'Metric':<20} {'Before SFT':<15} {'After SFT':<15} {'Change':<15}\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "acc_change = post_sft_results['accuracy'] - pre_sft_results['accuracy']\n",
        "partial_change = post_sft_results['partial_accuracy'] - pre_sft_results['partial_accuracy']\n",
        "format_change = post_sft_results['format_accuracy'] - pre_sft_results['format_accuracy']\n",
        "\n",
        "print(f\"{'Answer Accuracy':<20} {pre_sft_results['accuracy']:>6.2f}%      {post_sft_results['accuracy']:>6.2f}%      {acc_change:>+6.2f}%\")\n",
        "print(f\"{'Partial Accuracy':<20} {pre_sft_results['partial_accuracy']:>6.2f}%      {post_sft_results['partial_accuracy']:>6.2f}%      {partial_change:>+6.2f}%\")\n",
        "print(f\"{'Format Accuracy':<20} {pre_sft_results['format_accuracy']:>6.2f}%      {post_sft_results['format_accuracy']:>6.2f}%      {format_change:>+6.2f}%\")\n",
        "print(\"=\" * 65)\n",
        "\n",
        "print(\"\\nKey Improvements:\")\n",
        "if format_change > 0:\n",
        "    print(f\"  âœ“ Format accuracy improved by {format_change:.2f}%\")\n",
        "if acc_change > 0:\n",
        "    print(f\"  âœ“ Answer accuracy improved by {acc_change:.2f}%\")\n",
        "if acc_change <= 0 and format_change <= 0:\n",
        "    print(\"  Note: Model is learning the format, accuracy improvements may come in GRPO stage\")\n",
        "print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "print(\"\\nTensorBoard Visualization:\")\n",
        "print(f\"  View detailed metrics: tensorboard --logdir {sft_log_dir}\")\n",
        "print(\"  Or use: %tensorboard --logdir ./logs/sft\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 7: Stage 2 - GRPO Training\n",
        "\n",
        "### What is GRPO?\n",
        "\n",
        "GRPO (Group Relative Policy Optimization) is a reinforcement learning algorithm that:\n",
        "1. Generates multiple responses for each prompt\n",
        "2. Scores each response using reward functions\n",
        "3. Updates the model to favor higher-scoring responses\n",
        "\n",
        "Key advantages over PPO:\n",
        "- No need for a separate value/critic model (saves memory!)\n",
        "- Uses group-based advantage estimation\n",
        "- More stable training for reasoning tasks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 13: Define Reward Functions for GRPO\n",
        "# ============================================================================\n",
        "# These functions evaluate model outputs and provide learning signals.\n",
        "\n",
        "def match_format_exactly(prompts, completions, **kwargs):\n",
        "    \"\"\"\n",
        "    Reward function 1: Exact format matching\n",
        "    \n",
        "    Awards 3 points if the output exactly matches the expected format:\n",
        "    <reasoning>...</reasoning><answer>...</answer>\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "    for response in completions:\n",
        "        match = match_format.search(response)\n",
        "        rewards.append(3.0 if match else 0.0)\n",
        "    return rewards\n",
        "\n",
        "\n",
        "def match_format_approximately(prompts, completions, **kwargs):\n",
        "    \"\"\"\n",
        "    Reward function 2: Partial format matching\n",
        "    \n",
        "    Awards partial credit for having some of the required elements.\n",
        "    This helps the model learn incrementally.\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "    for completion in completions:\n",
        "        score = 0\n",
        "        # Check for each required element\n",
        "        score += 0.5 if completion.count(reasoning_start) == 1 else -0.5\n",
        "        score += 0.5 if completion.find(reasoning_start) == 0 else -0.5\n",
        "        score += 0.5 if completion.count(reasoning_end) == 1 else -0.5\n",
        "        score += 0.5 if completion.count(solution_start) == 1 else -0.5\n",
        "        score += 0.5 if completion.count(solution_end) == 1 else -0.5\n",
        "        scores.append(score)\n",
        "    return scores\n",
        "\n",
        "\n",
        "def check_answer(prompts, completions, answer, **kwargs):\n",
        "    \"\"\"\n",
        "    Reward function 3: Answer correctness\n",
        "    \n",
        "    Awards points based on how close the predicted answer is to the truth:\n",
        "    - 3.0 points for exact match\n",
        "    - 1.5 points for match after stripping whitespace\n",
        "    - 0.5 points if within 10% of correct value\n",
        "    - Penalties for wrong answers\n",
        "    \"\"\"\n",
        "    extracted_responses = [\n",
        "        guess.group(1) if r is not None and (guess := match_format.search(r)) else None\n",
        "        for r in completions\n",
        "    ]\n",
        "    \n",
        "    scores = []\n",
        "    for guess, true_answer in zip(extracted_responses, answer):\n",
        "        score = 0\n",
        "        if guess is None:\n",
        "            scores.append(0)\n",
        "            continue\n",
        "            \n",
        "        if guess == true_answer:\n",
        "            score += 3.0\n",
        "        elif guess.strip() == true_answer.strip():\n",
        "            score += 1.5\n",
        "        else:\n",
        "            try:\n",
        "                ratio = float(guess) / float(true_answer)\n",
        "                if 0.9 <= ratio <= 1.1:\n",
        "                    score += 0.5\n",
        "                elif 0.8 <= ratio <= 1.2:\n",
        "                    score += 0.25\n",
        "                else:\n",
        "                    score -= 1.0\n",
        "            except:\n",
        "                score -= 0.5\n",
        "        scores.append(score)\n",
        "    return scores\n",
        "\n",
        "\n",
        "def check_numbers(prompts, completions, answer, **kwargs):\n",
        "    \"\"\"\n",
        "    Reward function 4: Numerical answer extraction\n",
        "    \n",
        "    Sometimes the answer tag contains extra text. This function\n",
        "    extracts numerical values and compares them.\n",
        "    \"\"\"\n",
        "    question = kwargs.get(\"question\", [\"\"] * len(completions))\n",
        "    \n",
        "    extracted_responses = [\n",
        "        guess.group(1) if (guess := match_numbers.search(r)) else None\n",
        "        for r in completions\n",
        "    ]\n",
        "    \n",
        "    scores = []\n",
        "    # Log first example for debugging\n",
        "    if len(completions) > 0:\n",
        "        print(f\"Q: {question[0][:50]}... | A: {answer[0]} | Pred: {extracted_responses[0]}\")\n",
        "    \n",
        "    for guess, true_answer in zip(extracted_responses, answer):\n",
        "        if guess is None:\n",
        "            scores.append(0)\n",
        "            continue\n",
        "        try:\n",
        "            true_val = float(true_answer.strip())\n",
        "            pred_val = float(guess.strip())\n",
        "            scores.append(1.5 if pred_val == true_val else 0.0)\n",
        "        except:\n",
        "            scores.append(0)\n",
        "    return scores\n",
        "\n",
        "\n",
        "print(\"Reward functions defined:\")\n",
        "print(\"  1. match_format_exactly: Rewards correct output structure\")\n",
        "print(\"  2. match_format_approximately: Partial credit for format\")\n",
        "print(\"  3. check_answer: Rewards correct answers\")\n",
        "print(\"  4. check_numbers: Extracts and compares numerical answers\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 14: Prepare GRPO Dataset\n",
        "# ============================================================================\n",
        "\n",
        "print(\"Loading GRPO training data...\")\n",
        "\n",
        "# Load GSM8K dataset for GRPO training\n",
        "NUM_GRPO_BATCHES = 3738  # Full dataset\n",
        "NUM_TEST_BATCHES = 64    # For evaluation\n",
        "\n",
        "grpo_dataset = get_grpo_dataset(TRAIN_DATA_DIR, \"train\", \"kaggle\")\n",
        "grpo_dataset = grpo_dataset.batch(GRPO_BATCH_SIZE)[:NUM_GRPO_BATCHES]\n",
        "\n",
        "# Split into train/validation\n",
        "if TRAIN_FRACTION < 1.0:\n",
        "    split_idx = int(len(grpo_dataset) * TRAIN_FRACTION)\n",
        "    train_dataset = grpo_dataset[:split_idx]\n",
        "    val_dataset = grpo_dataset[split_idx:]\n",
        "else:\n",
        "    train_dataset = grpo_dataset\n",
        "    val_dataset = None\n",
        "\n",
        "# Load test dataset\n",
        "test_dataset = get_grpo_dataset(TEST_DATA_DIR, \"test\", \"kaggle\")\n",
        "test_dataset = test_dataset.batch(GRPO_BATCH_SIZE)[:NUM_TEST_BATCHES]\n",
        "\n",
        "print(f\"\\nDataset sizes:\")\n",
        "print(f\"  Training batches: {len(train_dataset)}\")\n",
        "print(f\"  Validation batches: {len(val_dataset) if val_dataset else 0}\")\n",
        "print(f\"  Test batches: {len(test_dataset)}\")\n",
        "\n",
        "# Preview a sample\n",
        "print(\"\\nSample GRPO training example:\")\n",
        "for batch in train_dataset[:1]:\n",
        "    pprint(batch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 15: Configure and Run GRPO Training\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"STAGE 2: GRPO REINFORCEMENT LEARNING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Calculate training steps\n",
        "GRPO_WARMUP_STEPS = int(GRPO_WARMUP_RATIO * GRPO_MAX_STEPS)\n",
        "EVAL_EVERY_N_STEPS = 64\n",
        "\n",
        "# Configure GRPO optimizer (lower learning rate than SFT!)\n",
        "grpo_optimizer = optax.adamw(\n",
        "    learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n",
        "        init_value=0.0,\n",
        "        peak_value=GRPO_LEARNING_RATE,\n",
        "        warmup_steps=GRPO_WARMUP_STEPS,\n",
        "        decay_steps=GRPO_MAX_STEPS,\n",
        "        end_value=0.0,\n",
        "    ),\n",
        "    b1=B1,\n",
        "    b2=B2,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        ")\n",
        "\n",
        "# Add gradient clipping (critical for RL stability!)\n",
        "grpo_optimizer = optax.chain(\n",
        "    optax.clip_by_global_norm(max_norm=MAX_GRAD_NORM),\n",
        "    grpo_optimizer,\n",
        ")\n",
        "\n",
        "# Checkpoint configuration\n",
        "grpo_ckpt_options = ocp.CheckpointManagerOptions(\n",
        "    save_interval_steps=SAVE_INTERVAL_STEPS,\n",
        "    max_to_keep=MAX_TO_KEEP,\n",
        ")\n",
        "\n",
        "# Metrics logging\n",
        "grpo_metrics_options = metrics_logger.MetricsLoggerOptions(\n",
        "    log_dir=\"./logs/grpo/\",\n",
        "    flush_every_n_steps=20,\n",
        ")\n",
        "\n",
        "print(f\"\\nGRPO Configuration:\")\n",
        "print(f\"  Learning rate: {GRPO_LEARNING_RATE}\")\n",
        "print(f\"  Generations per prompt (G): {NUM_GENERATIONS}\")\n",
        "print(f\"  KL penalty (beta): {BETA}\")\n",
        "print(f\"  Clipping (epsilon): {EPSILON}\")\n",
        "print(f\"  Max steps: {GRPO_MAX_STEPS}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 16: Initialize GRPO Cluster and Trainer\n",
        "# ============================================================================\n",
        "\n",
        "# Training configuration\n",
        "cluster_config = rl_cluster_lib.ClusterConfig(\n",
        "    role_to_mesh={\n",
        "        rl_cluster_lib.Role.ACTOR: mesh,\n",
        "        rl_cluster_lib.Role.REFERENCE: mesh,\n",
        "        rl_cluster_lib.Role.ROLLOUT: mesh,\n",
        "    },\n",
        "    rollout_engine='vanilla',\n",
        "    offload_to_cpu=False,\n",
        "    training_config=rl_cluster_lib.RLTrainingConfig(\n",
        "        actor_optimizer=grpo_optimizer,\n",
        "        eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
        "        max_steps=GRPO_MAX_STEPS,\n",
        "        mini_batch_size=GRPO_BATCH_SIZE,\n",
        "        train_micro_batch_size=GRPO_BATCH_SIZE,\n",
        "        metrics_logging_options=grpo_metrics_options,\n",
        "        checkpoint_root_directory=GRPO_CKPT_DIR,\n",
        "        checkpointing_options=grpo_ckpt_options,\n",
        "    ),\n",
        "    rollout_config=base_rollout.RolloutConfig(\n",
        "        max_tokens_to_generate=TOTAL_GENERATION_STEPS,\n",
        "        max_prompt_length=MAX_PROMPT_LENGTH,\n",
        "        kv_cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
        "        temperature=TEMPERATURE,\n",
        "        top_p=TOP_P,\n",
        "        top_k=TOP_K,\n",
        "        eos_tokens=EOS_TOKENS,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# GRPO algorithm configuration\n",
        "grpo_config = GRPOConfig(\n",
        "    num_generations=NUM_GENERATIONS,\n",
        "    num_iterations=NUM_ITERATIONS,\n",
        "    beta=BETA,\n",
        "    epsilon=EPSILON,\n",
        ")\n",
        "\n",
        "print(\"\\nInitializing RL Cluster...\")\n",
        "# Create RL cluster with policy and reference models\n",
        "rl_cluster = rl_cluster_lib.RLCluster(\n",
        "    actor=lora_policy,      # Model being trained (with LoRA)\n",
        "    reference=gemma3,        # Fixed reference model (for KL divergence)\n",
        "    tokenizer=tokenizer,\n",
        "    cluster_config=cluster_config,\n",
        ")\n",
        "\n",
        "print(\"Creating GRPO Trainer...\")\n",
        "# Create GRPO trainer with reward functions\n",
        "grpo_trainer = GRPOLearner(\n",
        "    rl_cluster=rl_cluster,\n",
        "    reward_fns=[\n",
        "        match_format_exactly,\n",
        "        match_format_approximately,\n",
        "        check_answer,\n",
        "        check_numbers,\n",
        "    ],\n",
        "    algo_config=grpo_config,\n",
        ")\n",
        "\n",
        "print(\"GRPO Trainer initialized successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 17: Launch TensorBoard for Monitoring\n",
        "# ============================================================================\n",
        "\n",
        "# Load TensorBoard extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Start TensorBoard to monitor training metrics\n",
        "%tensorboard --logdir ./logs/ --port=0\n",
        "\n",
        "print(\"TensorBoard launched! Monitor training metrics above.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 18: Run GRPO Training\n",
        "# ============================================================================\n",
        "# This is the main training loop. It may take several hours to complete.\n",
        "# Checkpoints are saved automatically during training.\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STARTING GRPO TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nThis will take approximately 6-8 hours on TPU v6e-1\")\n",
        "print(f\"Checkpoints saved every {SAVE_INTERVAL_STEPS} steps to {GRPO_CKPT_DIR}\")\n",
        "print(\"\\nMonitor progress in TensorBoard above.\")\n",
        "print(\"Key metrics to watch:\")\n",
        "print(\"  - reward_accuracy: Should increase over time\")\n",
        "print(\"  - reward_format: Should stay high (near 1.0)\")\n",
        "print(\"  - completion_length: May increase as model reasons more\")\n",
        "print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "# Run GRPO training\n",
        "grpo_trainer.train(train_dataset, val_dataset)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"GRPO TRAINING COMPLETE!\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 8: Evaluation\n",
        "\n",
        "Now we evaluate the trained model to see how much it has improved. We measure:\n",
        "1. **Answer Accuracy**: Percentage of correct answers\n",
        "2. **Partial Accuracy**: Answers within 10% of correct value  \n",
        "3. **Format Accuracy**: Outputs following the correct structure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 19: Load Best Checkpoint for Evaluation\n",
        "# ============================================================================\n",
        "\n",
        "# Load the trained checkpoint\n",
        "EVAL_CHECKPOINT_STEP = GRPO_MAX_STEPS  # Use final checkpoint\n",
        "\n",
        "trained_ckpt_path = os.path.join(\n",
        "    GRPO_CKPT_DIR, \"actor\", str(EVAL_CHECKPOINT_STEP), \"model_params\"\n",
        ")\n",
        "\n",
        "print(f\"Loading checkpoint from: {trained_ckpt_path}\")\n",
        "\n",
        "# Get the shape/dtype structure for restoration\n",
        "abs_params = jax.tree.map(\n",
        "    lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n",
        "    nnx.state(lora_policy, nnx.LoRAParam),\n",
        ")\n",
        "\n",
        "# Restore checkpoint\n",
        "checkpointer = ocp.StandardCheckpointer()\n",
        "trained_lora_params = checkpointer.restore(trained_ckpt_path, target=abs_params)\n",
        "\n",
        "# Update model with trained parameters\n",
        "nnx.update(\n",
        "    lora_policy,\n",
        "    jax.tree.map(\n",
        "        lambda a, b: b,\n",
        "        nnx.state(lora_policy, nnx.LoRAParam),\n",
        "        trained_lora_params,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"Checkpoint loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 20: Evaluation Functions\n",
        "# ============================================================================\n",
        "\n",
        "def generate(question, sampler, temperature=0.7, top_k=50, top_p=0.95, seed=None):\n",
        "    \"\"\"Generate response for a given question.\"\"\"\n",
        "    if isinstance(question, str):\n",
        "        input_batch = [TEMPLATE.format(system_prompt=SYSTEM_PROMPT, question=question)]\n",
        "    else:\n",
        "        input_batch = [\n",
        "            TEMPLATE.format(system_prompt=SYSTEM_PROMPT, question=q)\n",
        "            for q in question\n",
        "        ]\n",
        "    \n",
        "    out_data = sampler(\n",
        "        input_strings=input_batch,\n",
        "        max_generation_steps=TOTAL_GENERATION_STEPS,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        echo=False,\n",
        "        seed=seed,\n",
        "        eos_tokens=EOS_TOKENS,\n",
        "    )\n",
        "    \n",
        "    output = out_data.text\n",
        "    return output[0] if isinstance(question, str) else output\n",
        "\n",
        "\n",
        "def evaluate(dataset, sampler, temperature=0.7, top_k=50, top_p=0.95):\n",
        "    \"\"\"Evaluate model on dataset and compute metrics.\"\"\"\n",
        "    correct = 0\n",
        "    partially_correct = 0\n",
        "    correct_format = 0\n",
        "    total = 0\n",
        "    \n",
        "    for batch in tqdm(dataset, desc=\"Evaluating\"):\n",
        "        answers = batch[\"answer\"]\n",
        "        questions = batch[\"question\"]\n",
        "        \n",
        "        responses = generate(questions, sampler, temperature, top_k, top_p)\n",
        "        \n",
        "        for question, response, answer in zip(questions, responses, answers):\n",
        "            # Check answer\n",
        "            extracted = match_numbers.search(response)\n",
        "            if extracted:\n",
        "                try:\n",
        "                    pred = float(extracted.group(1).strip())\n",
        "                    true_val = float(answer.strip())\n",
        "                    if pred == true_val:\n",
        "                        correct += 1\n",
        "                    ratio = pred / true_val if true_val != 0 else 0\n",
        "                    if 0.9 <= ratio <= 1.1:\n",
        "                        partially_correct += 1\n",
        "                except:\n",
        "                    pass\n",
        "            \n",
        "            # Check format\n",
        "            if match_format.search(response):\n",
        "                correct_format += 1\n",
        "            \n",
        "            total += 1\n",
        "            \n",
        "            # Print progress\n",
        "            if total % 20 == 0:\n",
        "                print(f\"Progress: {correct}/{total} correct ({correct/total*100:.1f}%)\")\n",
        "    \n",
        "    return {\n",
        "        \"accuracy\": correct / total * 100,\n",
        "        \"partial_accuracy\": partially_correct / total * 100,\n",
        "        \"format_accuracy\": correct_format / total * 100,\n",
        "        \"correct\": correct,\n",
        "        \"total\": total,\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"Evaluation functions defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 21: Run Evaluation on Test Set\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"EVALUATING TRAINED MODEL\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create sampler for inference\n",
        "sampler = sampler_lib.Sampler(\n",
        "    transformer=lora_policy,\n",
        "    tokenizer=tokenizer,\n",
        "    cache_config=sampler_lib.CacheConfig(\n",
        "        cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
        "        num_layers=model_config.num_layers,\n",
        "        num_kv_heads=model_config.num_kv_heads,\n",
        "        head_dim=model_config.head_dim,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Run evaluation with greedy decoding for deterministic results\n",
        "print(\"\\nEvaluating with greedy decoding...\")\n",
        "results = evaluate(test_dataset, sampler, **GENERATION_CONFIGS[\"greedy\"])\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"EVALUATION RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"  Answer Accuracy:    {results['accuracy']:.2f}%\")\n",
        "print(f\"  Partial Accuracy:   {results['partial_accuracy']:.2f}%\")\n",
        "print(f\"  Format Accuracy:    {results['format_accuracy']:.2f}%\")\n",
        "print(f\"  Correct/Total:      {results['correct']}/{results['total']}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 9: Export Model\n",
        "\n",
        "Export the trained model in a format compatible with Tunix on Kaggle. The exported model:\n",
        "- Merges LoRA weights with the base model\n",
        "- Saves in safetensors format\n",
        "- Includes all necessary config files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 22: Export Merged Model (HuggingFace Format)\n",
        "# ============================================================================\n",
        "\n",
        "output_dir = FINAL_MODEL_DIR\n",
        "if USE_COLAB:\n",
        "    output_dir = f\"/tmp/content/{MODEL_ID.replace('/', '-')}-trained\"\n",
        "\n",
        "# Clean up existing output directory\n",
        "if os.path.exists(output_dir):\n",
        "    shutil.rmtree(output_dir)\n",
        "os.makedirs(output_dir)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"EXPORTING TRAINED MODEL\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nOutput directory: {output_dir}\")\n",
        "\n",
        "# Merge LoRA weights and save\n",
        "print(\"\\nMerging LoRA weights with base model...\")\n",
        "gemma_params.save_lora_merged_model_as_safetensors(\n",
        "    local_model_path=local_model_path,\n",
        "    output_dir=output_dir,\n",
        "    lora_model=lora_policy,\n",
        "    rank=RANK,\n",
        "    alpha=ALPHA,\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"MODEL EXPORTED SUCCESSFULLY!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# List saved files\n",
        "print(\"\\nSaved files:\")\n",
        "for f in sorted(os.listdir(output_dir)):\n",
        "    size = os.path.getsize(os.path.join(output_dir, f)) / (1024 * 1024)\n",
        "    print(f\"  {f:<35} {size:>10.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 23: Interactive Demo - Test the Trained Model\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"INTERACTIVE DEMO\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Test questions covering different domains\n",
        "test_questions = [\n",
        "    # Math\n",
        "    \"If a train travels at 60 miles per hour for 2.5 hours, how far does it travel?\",\n",
        "    # Logic\n",
        "    \"A farmer has 17 sheep. All but 9 die. How many sheep are left?\",\n",
        "    # Word problem\n",
        "    \"Lisa has 3 times as many apples as Tom. If Tom has 5 apples, how many do they have together?\",\n",
        "]\n",
        "\n",
        "print(\"\\nGenerating responses for test questions...\\n\")\n",
        "\n",
        "for i, question in enumerate(test_questions, 1):\n",
        "    print(f\"Question {i}: {question}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    response = generate(question, sampler, **GENERATION_CONFIGS[\"greedy\"])\n",
        "    print(f\"Response:\\n{response}\")\n",
        "    print(\"=\" * 60 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 24: Download Model (For Colab Users)\n",
        "# ============================================================================\n",
        "\n",
        "if USE_COLAB:\n",
        "    from google.colab import files\n",
        "    \n",
        "    # Create zip archive of the model\n",
        "    zip_path = f\"{output_dir}.zip\"\n",
        "    print(f\"Creating zip archive: {zip_path}\")\n",
        "    shutil.make_archive(output_dir, 'zip', output_dir)\n",
        "    \n",
        "    print(\"Downloading model... (this may take a moment)\")\n",
        "    files.download(zip_path)\n",
        "    print(\"Download complete!\")\n",
        "else:\n",
        "    print(f\"Model saved to: {output_dir}\")\n",
        "    print(\"\\nTo use this model, load it with Tunix:\")\n",
        "    print(f\"  model = params_safetensors_lib.create_model_from_safe_tensors('{output_dir}', config, mesh)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "Congratulations! You have successfully trained a reasoning model using:\n",
        "\n",
        "1. **Cold Start SFT**: Taught the model the output format\n",
        "   - `<reasoning>model_thinking_trace</reasoning>`\n",
        "   - `<answer>model_answer</answer>`\n",
        "\n",
        "2. **GRPO Reinforcement Learning**: Strengthened reasoning abilities\n",
        "   - Used multiple reward functions\n",
        "   - No separate value model needed (memory efficient!)\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "- **Cold Start is Essential**: Without it, RL training can cause chaotic outputs\n",
        "- **Learning Rate Matters**: SFT uses ~2e-4, GRPO uses ~3e-6 (much lower!)\n",
        "- **Checkpoints are Crucial**: Save frequently to recover from failures\n",
        "- **Format Rewards Help**: Reward proper structure, not just correct answers\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. Try different datasets (Bespoke-Stratos-17k, OpenR1-Math-220k)\n",
        "2. Experiment with hyperparameters (NUM_GENERATIONS, BETA, EPSILON)\n",
        "3. Train for more steps for better results\n",
        "4. Evaluate on diverse domains (coding, science, creative writing)\n",
        "\n",
        "### Resources\n",
        "\n",
        "- [Tunix GitHub](https://github.com/google/tunix)\n",
        "- [GRPO Paper](https://arxiv.org/pdf/2402.03300)\n",
        "- [DeepSeek-R1 Technical Report](https://arxiv.org/abs/2401.02954)\n",
        "- [Bespoke-Stratos Dataset](https://huggingface.co/datasets/bespokelabs/Bespoke-Stratos-17k)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
